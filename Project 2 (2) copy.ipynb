{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Authors</th>\n",
       "      <th>PublishYear</th>\n",
       "      <th>PublishMonth</th>\n",
       "      <th>PublishDay</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Language</th>\n",
       "      <th>pagesNumber</th>\n",
       "      <th>Description</th>\n",
       "      <th>rating_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best of Dr Jean: Reading &amp; Writing</td>\n",
       "      <td>Jean R. Feldman</td>\n",
       "      <td>2005</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Teaching Resources</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>Teachers will turn to this treasury of ideas a...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here All Dwell Free</td>\n",
       "      <td>Gertrud Mueller Nelson</td>\n",
       "      <td>1991</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>DoubleDay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>364</td>\n",
       "      <td>Every human being lives a fairy tale -- an unc...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boomer's Big Surprise</td>\n",
       "      <td>Constance W. McGeorge</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>Chronicle Books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;i&gt;Boomer's Big Surprise&lt;/i&gt; will have special...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'll Go and Do More: Annie Dodge Wauneka, Nava...</td>\n",
       "      <td>Carolyn Niethammer</td>\n",
       "      <td>2004</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Bison Books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>293</td>\n",
       "      <td>&lt;i&gt;I'll Go and Do More&lt;/i&gt; is the story of Ann...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Us</td>\n",
       "      <td>Richard       Mason</td>\n",
       "      <td>2005</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Penguin Books Ltd</td>\n",
       "      <td>eng</td>\n",
       "      <td>352</td>\n",
       "      <td>Since their days at Oxford, they've gone their...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name                 Authors  \\\n",
       "0                 Best of Dr Jean: Reading & Writing         Jean R. Feldman   \n",
       "1                                Here All Dwell Free  Gertrud Mueller Nelson   \n",
       "2                              Boomer's Big Surprise   Constance W. McGeorge   \n",
       "3  I'll Go and Do More: Annie Dodge Wauneka, Nava...      Carolyn Niethammer   \n",
       "4                                                 Us     Richard       Mason   \n",
       "\n",
       "   PublishYear  PublishMonth  PublishDay           Publisher Language  \\\n",
       "0         2005             6           1  Teaching Resources      NaN   \n",
       "1         1991            10           1           DoubleDay      NaN   \n",
       "2         2005             3          31     Chronicle Books      NaN   \n",
       "3         2004             9           1         Bison Books      NaN   \n",
       "4         2005             7           7   Penguin Books Ltd      eng   \n",
       "\n",
       "   pagesNumber                                        Description  \\\n",
       "0           48  Teachers will turn to this treasury of ideas a...   \n",
       "1          364  Every human being lives a fairy tale -- an unc...   \n",
       "2           32  <i>Boomer's Big Surprise</i> will have special...   \n",
       "3          293  <i>I'll Go and Do More</i> is the story of Ann...   \n",
       "4          352  Since their days at Oxford, they've gone their...   \n",
       "\n",
       "   rating_label  \n",
       "0           4.0  \n",
       "1           4.0  \n",
       "2           4.0  \n",
       "3           4.0  \n",
       "4           3.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the training and testing files\n",
    "x_train = pd.read_csv(r\"project_data_files/book_rating_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "x_test = pd.read_csv(r\"project_data_files/book_rating_test.csv\", index_col = False, delimiter = ',', header=0)\n",
    "\n",
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the doc2vec form of the text features in the dataset\n",
    "train_name_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/train_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_authors_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/train_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "train_desc_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/train_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "\n",
    "test_name_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/test_name_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_authors_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/test_authors_doc2vec20.csv\", index_col = False, delimiter = ',', header=None)\n",
    "test_desc_vec = pd.read_csv(r\"project_data_files/book_text_features_doc2vec/test_desc_doc2vec100.csv\", index_col = False, delimiter = ',', header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Anonymous              49\n",
       "William Shakespeare    48\n",
       "Carole Mortimer        47\n",
       "Nora Roberts           47\n",
       "Agatha Christie        46\n",
       "                       ..\n",
       "Arina Tanemura          1\n",
       "Linda Morse             1\n",
       "Richard W. Dortch       1\n",
       "Carol Allain            1\n",
       "Henry Rollins           1\n",
       "Name: Authors, Length: 16301, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of rating label\n",
    "rating_distribution = x_train['rating_label'].value_counts()\n",
    "rating_distribution\n",
    "\n",
    "languages = x_train['Language'].value_counts(dropna=False)\n",
    "languages\n",
    "\n",
    "publishers = x_train['Publisher'].value_counts(dropna=False)\n",
    "publishers\n",
    "\n",
    "authors = x_train['Authors'].value_counts(dropna=False)\n",
    "authors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Authors</th>\n",
       "      <th>PublishYear</th>\n",
       "      <th>PublishMonth</th>\n",
       "      <th>PublishDay</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Language</th>\n",
       "      <th>pagesNumber</th>\n",
       "      <th>Description</th>\n",
       "      <th>rating_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best of Dr Jean: Reading &amp; Writing</td>\n",
       "      <td>Jean R. Feldman</td>\n",
       "      <td>2005</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Teaching Resources</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>48</td>\n",
       "      <td>Teachers will turn to this treasury of ideas a...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here All Dwell Free</td>\n",
       "      <td>Gertrud Mueller Nelson</td>\n",
       "      <td>1991</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>DoubleDay</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>364</td>\n",
       "      <td>Every human being lives a fairy tale -- an unc...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boomer's Big Surprise</td>\n",
       "      <td>Constance W. McGeorge</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>Chronicle Books</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;i&gt;Boomer's Big Surprise&lt;/i&gt; will have special...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'll Go and Do More: Annie Dodge Wauneka, Nava...</td>\n",
       "      <td>Carolyn Niethammer</td>\n",
       "      <td>2004</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Bison Books</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>293</td>\n",
       "      <td>&lt;i&gt;I'll Go and Do More&lt;/i&gt; is the story of Ann...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Us</td>\n",
       "      <td>Richard       Mason</td>\n",
       "      <td>2005</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Penguin Books Ltd</td>\n",
       "      <td>eng</td>\n",
       "      <td>352</td>\n",
       "      <td>Since their days at Oxford, they've gone their...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name                 Authors  \\\n",
       "0                 Best of Dr Jean: Reading & Writing         Jean R. Feldman   \n",
       "1                                Here All Dwell Free  Gertrud Mueller Nelson   \n",
       "2                              Boomer's Big Surprise   Constance W. McGeorge   \n",
       "3  I'll Go and Do More: Annie Dodge Wauneka, Nava...      Carolyn Niethammer   \n",
       "4                                                 Us     Richard       Mason   \n",
       "\n",
       "   PublishYear  PublishMonth  PublishDay           Publisher Language  \\\n",
       "0         2005             6           1  Teaching Resources  Unknown   \n",
       "1         1991            10           1           DoubleDay  Unknown   \n",
       "2         2005             3          31     Chronicle Books  Unknown   \n",
       "3         2004             9           1         Bison Books  Unknown   \n",
       "4         2005             7           7   Penguin Books Ltd      eng   \n",
       "\n",
       "   pagesNumber                                        Description  \\\n",
       "0           48  Teachers will turn to this treasury of ideas a...   \n",
       "1          364  Every human being lives a fairy tale -- an unc...   \n",
       "2           32  <i>Boomer's Big Surprise</i> will have special...   \n",
       "3          293  <i>I'll Go and Do More</i> is the story of Ann...   \n",
       "4          352  Since their days at Oxford, they've gone their...   \n",
       "\n",
       "   rating_label  \n",
       "0           4.0  \n",
       "1           4.0  \n",
       "2           4.0  \n",
       "3           4.0  \n",
       "4           3.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deal with blank feature values\n",
    "x_train.fillna('Unknown', inplace=True)\n",
    "y_train = x_train['rating_label']\n",
    "\n",
    "x_test.fillna('Unknown', inplace=True)\n",
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unknown</th>\n",
       "      <th>ara</th>\n",
       "      <th>eng</th>\n",
       "      <th>fre</th>\n",
       "      <th>frs</th>\n",
       "      <th>ger</th>\n",
       "      <th>grc</th>\n",
       "      <th>heb</th>\n",
       "      <th>ita</th>\n",
       "      <th>jpn</th>\n",
       "      <th>lat</th>\n",
       "      <th>mul</th>\n",
       "      <th>nld</th>\n",
       "      <th>per</th>\n",
       "      <th>por</th>\n",
       "      <th>rus</th>\n",
       "      <th>spa</th>\n",
       "      <th>swe</th>\n",
       "      <th>zho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23058</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23059</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23060</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23061</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23062</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23063 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unknown  ara  eng  fre  frs  ger  grc  heb  ita  jpn  lat  mul  nld  \\\n",
       "0            1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1            1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "2            1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "3            1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "4            0    0    1    0    0    0    0    0    0    0    0    0    0   \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "23058        0    0    1    0    0    0    0    0    0    0    0    0    0   \n",
       "23059        1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "23060        1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "23061        1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "23062        1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "       per  por  rus  spa  swe  zho  \n",
       "0        0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  \n",
       "23058    0    0    0    0    0    0  \n",
       "23059    0    0    0    0    0    0  \n",
       "23060    0    0    0    0    0    0  \n",
       "23061    0    0    0    0    0    0  \n",
       "23062    0    0    0    0    0    0  \n",
       "\n",
       "[23063 rows x 19 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform all the text-based features using either doc2vec or one-hot encoding\n",
    "train_publisher = pd.get_dummies(x_train['Publisher'])\n",
    "train_language = pd.get_dummies(x_train['Language'])\n",
    "train_publisher\n",
    "train_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unknown</th>\n",
       "      <th>eng</th>\n",
       "      <th>fre</th>\n",
       "      <th>ger</th>\n",
       "      <th>glg</th>\n",
       "      <th>grc</th>\n",
       "      <th>hun</th>\n",
       "      <th>ita</th>\n",
       "      <th>jpn</th>\n",
       "      <th>mul</th>\n",
       "      <th>nld</th>\n",
       "      <th>per</th>\n",
       "      <th>por</th>\n",
       "      <th>rus</th>\n",
       "      <th>spa</th>\n",
       "      <th>swe</th>\n",
       "      <th>tha</th>\n",
       "      <th>urd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5766 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unknown  eng  fre  ger  glg  grc  hun  ita  jpn  mul  nld  per  por  \\\n",
       "0           1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "1           1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "2           0    1    0    0    0    0    0    0    0    0    0    0    0   \n",
       "3           0    1    0    0    0    0    0    0    0    0    0    0    0   \n",
       "4           1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "5761        1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "5762        1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "5763        1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "5764        0    1    0    0    0    0    0    0    0    0    0    0    0   \n",
       "5765        1    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "      rus  spa  swe  tha  urd  \n",
       "0       0    0    0    0    0  \n",
       "1       0    0    0    0    0  \n",
       "2       0    0    0    0    0  \n",
       "3       0    0    0    0    0  \n",
       "4       0    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  \n",
       "5761    0    0    0    0    0  \n",
       "5762    0    0    0    0    0  \n",
       "5763    0    0    0    0    0  \n",
       "5764    0    0    0    0    0  \n",
       "5765    0    0    0    0    0  \n",
       "\n",
       "[5766 rows x 18 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_publisher = pd.get_dummies(x_test['Publisher'])\n",
    "test_language = pd.get_dummies(x_test['Language'])\n",
    "test_publisher\n",
    "test_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there are unseen feature values in the test set, make extra encoding\n",
    "# for both encoded \"publisher\" and \"language\" in training and testing sets\n",
    "train_publisher = pd.concat([train_publisher, \n",
    "pd.DataFrame(0, index=train_publisher.index, \n",
    "columns=[x for x in list(test_publisher.columns) if x not in list(train_publisher.columns)])], axis=1)\n",
    "\n",
    "test_publisher = pd.concat([test_publisher, \n",
    "pd.DataFrame(0, index=test_publisher.index, \n",
    "columns=[x for x in list(train_publisher.columns) if x not in list(test_publisher.columns)])], axis=1)\n",
    "\n",
    "train_language = pd.concat([train_language, \n",
    "pd.DataFrame(0, index=train_language.index, \n",
    "columns=[x for x in list(test_language.columns) if x not in list(train_language.columns)])], axis=1)\n",
    "\n",
    "test_language = pd.concat([test_language, \n",
    "pd.DataFrame(0, index=test_language.index, \n",
    "columns=[x for x in list(train_language.columns) if x not in list(test_language.columns)])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>توپ</th>\n",
       "      <th>نشر ثالث</th>\n",
       "      <th>هرمس</th>\n",
       "      <th>小学館</th>\n",
       "      <th>幻冬舎 / Gentōsha</th>\n",
       "      <th>遠流出版事業股份有限公司</th>\n",
       "      <th>pagesNumber</th>\n",
       "      <th>PublishYear</th>\n",
       "      <th>PublishMonth</th>\n",
       "      <th>PublishDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029333</td>\n",
       "      <td>-0.043927</td>\n",
       "      <td>0.102285</td>\n",
       "      <td>-0.121823</td>\n",
       "      <td>-0.016668</td>\n",
       "      <td>-0.137888</td>\n",
       "      <td>-0.059163</td>\n",
       "      <td>0.216625</td>\n",
       "      <td>-0.136272</td>\n",
       "      <td>-0.133504</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>1993</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138439</td>\n",
       "      <td>-0.003614</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>-0.061709</td>\n",
       "      <td>-0.035443</td>\n",
       "      <td>-0.106722</td>\n",
       "      <td>-0.017021</td>\n",
       "      <td>0.077577</td>\n",
       "      <td>-0.055408</td>\n",
       "      <td>-0.108035</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008959</td>\n",
       "      <td>-0.102902</td>\n",
       "      <td>0.100617</td>\n",
       "      <td>-0.217163</td>\n",
       "      <td>0.031681</td>\n",
       "      <td>-0.155196</td>\n",
       "      <td>-0.052115</td>\n",
       "      <td>-0.083774</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>-0.142214</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>544</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.103115</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>-0.104288</td>\n",
       "      <td>-0.078576</td>\n",
       "      <td>0.094428</td>\n",
       "      <td>-0.073938</td>\n",
       "      <td>0.090622</td>\n",
       "      <td>-0.018610</td>\n",
       "      <td>-0.014757</td>\n",
       "      <td>-0.043682</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>2006</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.377294</td>\n",
       "      <td>-0.037297</td>\n",
       "      <td>0.068795</td>\n",
       "      <td>-0.165371</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.058643</td>\n",
       "      <td>0.070002</td>\n",
       "      <td>-0.083565</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>-0.213105</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>352</td>\n",
       "      <td>2002</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>0.137909</td>\n",
       "      <td>0.155925</td>\n",
       "      <td>-0.041329</td>\n",
       "      <td>0.203567</td>\n",
       "      <td>-0.061073</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.058941</td>\n",
       "      <td>0.064054</td>\n",
       "      <td>0.197957</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>2003</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.013475</td>\n",
       "      <td>0.030343</td>\n",
       "      <td>-0.096524</td>\n",
       "      <td>0.074866</td>\n",
       "      <td>-0.025272</td>\n",
       "      <td>-0.014977</td>\n",
       "      <td>0.176195</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>2004</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>-0.176006</td>\n",
       "      <td>0.025466</td>\n",
       "      <td>-0.193973</td>\n",
       "      <td>-0.095170</td>\n",
       "      <td>0.077487</td>\n",
       "      <td>0.044431</td>\n",
       "      <td>0.068534</td>\n",
       "      <td>-0.316328</td>\n",
       "      <td>0.037170</td>\n",
       "      <td>0.198731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>1999</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>-0.011545</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.032473</td>\n",
       "      <td>0.103847</td>\n",
       "      <td>-0.124135</td>\n",
       "      <td>-0.163065</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.164405</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>-0.057711</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>1972</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>0.127689</td>\n",
       "      <td>0.296551</td>\n",
       "      <td>-0.047495</td>\n",
       "      <td>0.034397</td>\n",
       "      <td>0.035051</td>\n",
       "      <td>0.038985</td>\n",
       "      <td>-0.018085</td>\n",
       "      <td>-0.009776</td>\n",
       "      <td>0.029559</td>\n",
       "      <td>0.028496</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>1994</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5766 rows × 5052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.029333 -0.043927  0.102285 -0.121823 -0.016668 -0.137888 -0.059163   \n",
       "1     0.138439 -0.003614 -0.002051 -0.061709 -0.035443 -0.106722 -0.017021   \n",
       "2     0.008959 -0.102902  0.100617 -0.217163  0.031681 -0.155196 -0.052115   \n",
       "3    -0.103115  0.003076 -0.104288 -0.078576  0.094428 -0.073938  0.090622   \n",
       "4    -0.377294 -0.037297  0.068795 -0.165371  0.006160  0.058643  0.070002   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5761  0.137909  0.155925 -0.041329  0.203567 -0.061073 -0.004454  0.011992   \n",
       "5762 -0.019626 -0.073607 -0.013475  0.030343 -0.096524  0.074866 -0.025272   \n",
       "5763 -0.176006  0.025466 -0.193973 -0.095170  0.077487  0.044431  0.068534   \n",
       "5764 -0.011545  0.012677  0.032473  0.103847 -0.124135 -0.163065  0.016789   \n",
       "5765  0.127689  0.296551 -0.047495  0.034397  0.035051  0.038985 -0.018085   \n",
       "\n",
       "             7         8         9  ...  توپ  نشر ثالث  هرمس  小学館  \\\n",
       "0     0.216625 -0.136272 -0.133504  ...    0         0     0    0   \n",
       "1     0.077577 -0.055408 -0.108035  ...    0         0     0    0   \n",
       "2    -0.083774  0.000383 -0.142214  ...    0         0     0    0   \n",
       "3    -0.018610 -0.014757 -0.043682  ...    0         0     0    0   \n",
       "4    -0.083565  0.015256 -0.213105  ...    0         0     0    0   \n",
       "...        ...       ...       ...  ...  ...       ...   ...  ...   \n",
       "5761  0.058941  0.064054  0.197957  ...    0         0     0    0   \n",
       "5762 -0.014977  0.176195  0.061163  ...    0         0     0    0   \n",
       "5763 -0.316328  0.037170  0.198731  ...    0         0     0    0   \n",
       "5764  0.164405  0.060370 -0.057711  ...    0         0     0    0   \n",
       "5765 -0.009776  0.029559  0.028496  ...    0         0     0    0   \n",
       "\n",
       "      幻冬舎 / Gentōsha  遠流出版事業股份有限公司  pagesNumber  PublishYear  PublishMonth  \\\n",
       "0                  0             0          118         1993             1   \n",
       "1                  0             0           32         1997             3   \n",
       "2                  0             0          544         2005             1   \n",
       "3                  0             0          432         2006             9   \n",
       "4                  0             0          352         2002             5   \n",
       "...              ...           ...          ...          ...           ...   \n",
       "5761               0             0          274         2003             9   \n",
       "5762               0             0          224         2004             9   \n",
       "5763               0             0          224         1999            12   \n",
       "5764               0             0          242         1972             6   \n",
       "5765               0             0          192         1994             9   \n",
       "\n",
       "      PublishDay  \n",
       "0              1  \n",
       "1             17  \n",
       "2              1  \n",
       "3              5  \n",
       "4              3  \n",
       "...          ...  \n",
       "5761           1  \n",
       "5762          10  \n",
       "5763          14  \n",
       "5764          21  \n",
       "5765           1  \n",
       "\n",
       "[5766 rows x 5052 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all the processed features into one complete df for future uses\n",
    "# below is for testing df\n",
    "df_test = pd.concat([test_name_vec, test_authors_vec, test_desc_vec], axis=1)\n",
    "df_test.columns = range(len(df_test.columns))\n",
    "df_test = pd.concat([df_test, test_language, test_publisher], axis=1)\n",
    "df_test['pagesNumber'] = x_test['pagesNumber']\n",
    "df_test['PublishYear'] = x_test['PublishYear']\n",
    "df_test['PublishMonth'] = x_test['PublishMonth']\n",
    "df_test['PublishDay'] = x_test['PublishDay']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>Yoda Press</th>\n",
       "      <th>Zebra Historic Fantasy</th>\n",
       "      <th>Zebra Splendor</th>\n",
       "      <th>Zomba Books</th>\n",
       "      <th>dtv Deutscher Taschenbuchverlag</th>\n",
       "      <th>河出書房新社</th>\n",
       "      <th>pagesNumber</th>\n",
       "      <th>PublishYear</th>\n",
       "      <th>PublishMonth</th>\n",
       "      <th>PublishDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052262</td>\n",
       "      <td>-0.263308</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>0.128574</td>\n",
       "      <td>-0.161565</td>\n",
       "      <td>-0.127520</td>\n",
       "      <td>0.249588</td>\n",
       "      <td>0.037621</td>\n",
       "      <td>-0.074043</td>\n",
       "      <td>0.072854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2005</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.129112</td>\n",
       "      <td>0.021312</td>\n",
       "      <td>0.159166</td>\n",
       "      <td>-0.072448</td>\n",
       "      <td>0.036028</td>\n",
       "      <td>-0.093721</td>\n",
       "      <td>0.129199</td>\n",
       "      <td>0.069736</td>\n",
       "      <td>-0.253263</td>\n",
       "      <td>-0.066424</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "      <td>1991</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170058</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>-0.013406</td>\n",
       "      <td>0.099001</td>\n",
       "      <td>0.083173</td>\n",
       "      <td>-0.161439</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.063164</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250849</td>\n",
       "      <td>0.021555</td>\n",
       "      <td>0.091047</td>\n",
       "      <td>-0.041589</td>\n",
       "      <td>-0.040949</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.415056</td>\n",
       "      <td>0.027029</td>\n",
       "      <td>-0.172413</td>\n",
       "      <td>-0.135485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "      <td>2004</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041681</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>-0.051164</td>\n",
       "      <td>-0.076813</td>\n",
       "      <td>0.096855</td>\n",
       "      <td>-0.215943</td>\n",
       "      <td>0.152729</td>\n",
       "      <td>0.267636</td>\n",
       "      <td>-0.079954</td>\n",
       "      <td>-0.065560</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>352</td>\n",
       "      <td>2005</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23058</th>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.019723</td>\n",
       "      <td>-0.003321</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>-0.129420</td>\n",
       "      <td>0.130302</td>\n",
       "      <td>-0.037361</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>-0.255112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>1997</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23059</th>\n",
       "      <td>-0.024484</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.015977</td>\n",
       "      <td>0.086630</td>\n",
       "      <td>0.082127</td>\n",
       "      <td>-0.174537</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.111608</td>\n",
       "      <td>-0.106961</td>\n",
       "      <td>-0.147956</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>2005</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23060</th>\n",
       "      <td>-0.099309</td>\n",
       "      <td>-0.046230</td>\n",
       "      <td>-0.033294</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.055477</td>\n",
       "      <td>-0.033886</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>-0.126636</td>\n",
       "      <td>0.127742</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>1989</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23061</th>\n",
       "      <td>-0.038388</td>\n",
       "      <td>0.065679</td>\n",
       "      <td>-0.159324</td>\n",
       "      <td>-0.048682</td>\n",
       "      <td>0.054175</td>\n",
       "      <td>0.317751</td>\n",
       "      <td>0.065931</td>\n",
       "      <td>-0.126021</td>\n",
       "      <td>-0.105057</td>\n",
       "      <td>-0.147185</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>1998</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23062</th>\n",
       "      <td>-0.051934</td>\n",
       "      <td>-0.005339</td>\n",
       "      <td>0.030417</td>\n",
       "      <td>0.044278</td>\n",
       "      <td>-0.031339</td>\n",
       "      <td>-0.020412</td>\n",
       "      <td>0.038031</td>\n",
       "      <td>0.073363</td>\n",
       "      <td>-0.068429</td>\n",
       "      <td>-0.037205</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>2002</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23063 rows × 5052 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.052262 -0.263308  0.026872  0.128574 -0.161565 -0.127520  0.249588   \n",
       "1     -0.129112  0.021312  0.159166 -0.072448  0.036028 -0.093721  0.129199   \n",
       "2     -0.170058  0.052351 -0.013406  0.099001  0.083173 -0.161439  0.048635   \n",
       "3      0.250849  0.021555  0.091047 -0.041589 -0.040949  0.240260  0.415056   \n",
       "4     -0.041681  0.038051 -0.051164 -0.076813  0.096855 -0.215943  0.152729   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23058  0.007497  0.000220  0.019723 -0.003321  0.021097 -0.129420  0.130302   \n",
       "23059 -0.024484  0.000467 -0.015977  0.086630  0.082127 -0.174537  0.011694   \n",
       "23060 -0.099309 -0.046230 -0.033294  0.242591 -0.055477 -0.033886  0.026869   \n",
       "23061 -0.038388  0.065679 -0.159324 -0.048682  0.054175  0.317751  0.065931   \n",
       "23062 -0.051934 -0.005339  0.030417  0.044278 -0.031339 -0.020412  0.038031   \n",
       "\n",
       "              7         8         9  ...  Yoda Press  Zebra Historic Fantasy  \\\n",
       "0      0.037621 -0.074043  0.072854  ...           0                       0   \n",
       "1      0.069736 -0.253263 -0.066424  ...           0                       0   \n",
       "2      0.089419 -0.072266 -0.063164  ...           0                       0   \n",
       "3      0.027029 -0.172413 -0.135485  ...           0                       0   \n",
       "4      0.267636 -0.079954 -0.065560  ...           0                       0   \n",
       "...         ...       ...       ...  ...         ...                     ...   \n",
       "23058 -0.037361 -0.004281 -0.255112  ...           0                       0   \n",
       "23059  0.111608 -0.106961 -0.147956  ...           0                       0   \n",
       "23060  0.038410 -0.126636  0.127742  ...           0                       0   \n",
       "23061 -0.126021 -0.105057 -0.147185  ...           0                       0   \n",
       "23062  0.073363 -0.068429 -0.037205  ...           0                       0   \n",
       "\n",
       "       Zebra Splendor  Zomba Books  dtv Deutscher Taschenbuchverlag  河出書房新社  \\\n",
       "0                   0            0                                0       0   \n",
       "1                   0            0                                0       0   \n",
       "2                   0            0                                0       0   \n",
       "3                   0            0                                0       0   \n",
       "4                   0            0                                0       0   \n",
       "...               ...          ...                              ...     ...   \n",
       "23058               0            0                                0       0   \n",
       "23059               0            0                                0       0   \n",
       "23060               0            0                                0       0   \n",
       "23061               0            0                                0       0   \n",
       "23062               0            0                                0       0   \n",
       "\n",
       "       pagesNumber  PublishYear  PublishMonth  PublishDay  \n",
       "0               48         2005             6           1  \n",
       "1              364         1991            10           1  \n",
       "2               32         2005             3          31  \n",
       "3              293         2004             9           1  \n",
       "4              352         2005             7           7  \n",
       "...            ...          ...           ...         ...  \n",
       "23058          120         1997             8           1  \n",
       "23059           32         2005             6           1  \n",
       "23060          132         1989             2          15  \n",
       "23061          136         1998             4          21  \n",
       "23062          192         2002             7           8  \n",
       "\n",
       "[23063 rows x 5052 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below is for training df\n",
    "df_train = pd.concat([train_name_vec, train_authors_vec, train_desc_vec], axis=1)\n",
    "df_train.columns = range(len(df_train.columns))\n",
    "df_train = pd.concat([df_train, train_language, train_publisher], axis=1)\n",
    "df_train['pagesNumber'] = x_train['pagesNumber']\n",
    "df_train['PublishYear'] = x_train['PublishYear']\n",
    "df_train['PublishMonth'] = x_train['PublishMonth']\n",
    "df_train['PublishDay'] = x_train['PublishDay']\n",
    "df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, use information gain to see if there are any attributes not contributing\n",
    "# to the final labels of the book\n",
    "def entropy(feature):\n",
    "    prob = feature.value_counts(normalize=True)\n",
    "    entropy = -1 * np.sum(prob * np.log2(prob))       \n",
    "    return entropy\n",
    "\n",
    "def information_gain(data, label, feature):\n",
    "    print('Feature:', feature)\n",
    "    entropy_label = entropy(data[label])\n",
    "    entropy_list = list()\n",
    "    prob_list = list()\n",
    "    for i in data[feature].unique():\n",
    "        # Calculate H(feature)\n",
    "        feature_i = data[data[feature] == i]\n",
    "        entropy_i = entropy(feature_i[label])\n",
    "        entropy_list.append(entropy_i)\n",
    "        prob_i = len(feature_i) / len(data)\n",
    "        prob_list.append(prob_i)\n",
    "    # Calculate mean information: sum of P(value)*H(value)\n",
    "    mean_info = np.sum(np.array(entropy_list) * np.array(prob_list))\n",
    "    information_gain = entropy_label - mean_info\n",
    "    print('information gain:', information_gain)\n",
    "    print('====================')\n",
    "    return(information_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: PublishYear\n",
      "information gain: 0.014102821492525619\n",
      "====================\n",
      "Feature: PublishMonth\n",
      "information gain: 0.0016625179726277306\n",
      "====================\n",
      "Feature: PublishDay\n",
      "information gain: 0.003286848075712445\n",
      "====================\n",
      "Feature: Publisher\n",
      "information gain: 0.31951734938247933\n",
      "====================\n",
      "Feature: Language\n",
      "information gain: 0.008855268387619297\n",
      "====================\n",
      "Feature: pagesNumber\n",
      "information gain: 0.08896965611245478\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# authors have large branching factor, maybe later find some top authors\n",
    "# in different selection techniques\n",
    "for feature in x_train.drop(columns=['Name', 'Authors', 'Description', 'rating_label']).columns:\n",
    "    feature_info_gain = information_gain(x_train, y_train.name, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>Éditions Glénat</th>\n",
       "      <th>Éditions P.O.L</th>\n",
       "      <th>تنوير للنشر والإعلام</th>\n",
       "      <th>توپ</th>\n",
       "      <th>نشر ثالث</th>\n",
       "      <th>هرمس</th>\n",
       "      <th>小学館</th>\n",
       "      <th>幻冬舎 / Gentōsha</th>\n",
       "      <th>遠流出版事業股份有限公司</th>\n",
       "      <th>pagesNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029333</td>\n",
       "      <td>-0.043927</td>\n",
       "      <td>0.102285</td>\n",
       "      <td>-0.121823</td>\n",
       "      <td>-0.016668</td>\n",
       "      <td>-0.137888</td>\n",
       "      <td>-0.059163</td>\n",
       "      <td>0.216625</td>\n",
       "      <td>-0.136272</td>\n",
       "      <td>-0.133504</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138439</td>\n",
       "      <td>-0.003614</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>-0.061709</td>\n",
       "      <td>-0.035443</td>\n",
       "      <td>-0.106722</td>\n",
       "      <td>-0.017021</td>\n",
       "      <td>0.077577</td>\n",
       "      <td>-0.055408</td>\n",
       "      <td>-0.108035</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008959</td>\n",
       "      <td>-0.102902</td>\n",
       "      <td>0.100617</td>\n",
       "      <td>-0.217163</td>\n",
       "      <td>0.031681</td>\n",
       "      <td>-0.155196</td>\n",
       "      <td>-0.052115</td>\n",
       "      <td>-0.083774</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>-0.142214</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.103115</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>-0.104288</td>\n",
       "      <td>-0.078576</td>\n",
       "      <td>0.094428</td>\n",
       "      <td>-0.073938</td>\n",
       "      <td>0.090622</td>\n",
       "      <td>-0.018610</td>\n",
       "      <td>-0.014757</td>\n",
       "      <td>-0.043682</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.377294</td>\n",
       "      <td>-0.037297</td>\n",
       "      <td>0.068795</td>\n",
       "      <td>-0.165371</td>\n",
       "      <td>0.006160</td>\n",
       "      <td>0.058643</td>\n",
       "      <td>0.070002</td>\n",
       "      <td>-0.083565</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>-0.213105</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5761</th>\n",
       "      <td>0.137909</td>\n",
       "      <td>0.155925</td>\n",
       "      <td>-0.041329</td>\n",
       "      <td>0.203567</td>\n",
       "      <td>-0.061073</td>\n",
       "      <td>-0.004454</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.058941</td>\n",
       "      <td>0.064054</td>\n",
       "      <td>0.197957</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>-0.019626</td>\n",
       "      <td>-0.073607</td>\n",
       "      <td>-0.013475</td>\n",
       "      <td>0.030343</td>\n",
       "      <td>-0.096524</td>\n",
       "      <td>0.074866</td>\n",
       "      <td>-0.025272</td>\n",
       "      <td>-0.014977</td>\n",
       "      <td>0.176195</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>-0.176006</td>\n",
       "      <td>0.025466</td>\n",
       "      <td>-0.193973</td>\n",
       "      <td>-0.095170</td>\n",
       "      <td>0.077487</td>\n",
       "      <td>0.044431</td>\n",
       "      <td>0.068534</td>\n",
       "      <td>-0.316328</td>\n",
       "      <td>0.037170</td>\n",
       "      <td>0.198731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>-0.011545</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.032473</td>\n",
       "      <td>0.103847</td>\n",
       "      <td>-0.124135</td>\n",
       "      <td>-0.163065</td>\n",
       "      <td>0.016789</td>\n",
       "      <td>0.164405</td>\n",
       "      <td>0.060370</td>\n",
       "      <td>-0.057711</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5765</th>\n",
       "      <td>0.127689</td>\n",
       "      <td>0.296551</td>\n",
       "      <td>-0.047495</td>\n",
       "      <td>0.034397</td>\n",
       "      <td>0.035051</td>\n",
       "      <td>0.038985</td>\n",
       "      <td>-0.018085</td>\n",
       "      <td>-0.009776</td>\n",
       "      <td>0.029559</td>\n",
       "      <td>0.028496</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5766 rows × 5026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     0.029333 -0.043927  0.102285 -0.121823 -0.016668 -0.137888 -0.059163   \n",
       "1     0.138439 -0.003614 -0.002051 -0.061709 -0.035443 -0.106722 -0.017021   \n",
       "2     0.008959 -0.102902  0.100617 -0.217163  0.031681 -0.155196 -0.052115   \n",
       "3    -0.103115  0.003076 -0.104288 -0.078576  0.094428 -0.073938  0.090622   \n",
       "4    -0.377294 -0.037297  0.068795 -0.165371  0.006160  0.058643  0.070002   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5761  0.137909  0.155925 -0.041329  0.203567 -0.061073 -0.004454  0.011992   \n",
       "5762 -0.019626 -0.073607 -0.013475  0.030343 -0.096524  0.074866 -0.025272   \n",
       "5763 -0.176006  0.025466 -0.193973 -0.095170  0.077487  0.044431  0.068534   \n",
       "5764 -0.011545  0.012677  0.032473  0.103847 -0.124135 -0.163065  0.016789   \n",
       "5765  0.127689  0.296551 -0.047495  0.034397  0.035051  0.038985 -0.018085   \n",
       "\n",
       "             7         8         9  ...  Éditions Glénat  Éditions P.O.L  \\\n",
       "0     0.216625 -0.136272 -0.133504  ...                0               0   \n",
       "1     0.077577 -0.055408 -0.108035  ...                0               0   \n",
       "2    -0.083774  0.000383 -0.142214  ...                0               0   \n",
       "3    -0.018610 -0.014757 -0.043682  ...                0               0   \n",
       "4    -0.083565  0.015256 -0.213105  ...                0               0   \n",
       "...        ...       ...       ...  ...              ...             ...   \n",
       "5761  0.058941  0.064054  0.197957  ...                0               0   \n",
       "5762 -0.014977  0.176195  0.061163  ...                0               0   \n",
       "5763 -0.316328  0.037170  0.198731  ...                0               0   \n",
       "5764  0.164405  0.060370 -0.057711  ...                0               0   \n",
       "5765 -0.009776  0.029559  0.028496  ...                0               0   \n",
       "\n",
       "      تنوير للنشر والإعلام  توپ  نشر ثالث  هرمس  小学館  幻冬舎 / Gentōsha  \\\n",
       "0                        0    0         0     0    0               0   \n",
       "1                        0    0         0     0    0               0   \n",
       "2                        0    0         0     0    0               0   \n",
       "3                        0    0         0     0    0               0   \n",
       "4                        0    0         0     0    0               0   \n",
       "...                    ...  ...       ...   ...  ...             ...   \n",
       "5761                     0    0         0     0    0               0   \n",
       "5762                     0    0         0     0    0               0   \n",
       "5763                     0    0         0     0    0               0   \n",
       "5764                     0    0         0     0    0               0   \n",
       "5765                     0    0         0     0    0               0   \n",
       "\n",
       "      遠流出版事業股份有限公司  pagesNumber  \n",
       "0                0          118  \n",
       "1                0           32  \n",
       "2                0          544  \n",
       "3                0          432  \n",
       "4                0          352  \n",
       "...            ...          ...  \n",
       "5761             0          274  \n",
       "5762             0          224  \n",
       "5763             0          224  \n",
       "5764             0          242  \n",
       "5765             0          192  \n",
       "\n",
       "[5766 rows x 5026 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the above results, publish date and language seems to not contribute\n",
    "# too much to the final labels, so we can discard these features later in modelling\n",
    "df_test = pd.concat([test_name_vec, test_authors_vec, test_desc_vec], axis=1)\n",
    "df_test.columns = range(len(df_test.columns))\n",
    "df_test = pd.concat([df_test, test_publisher], axis=1)\n",
    "df_test['pagesNumber'] = x_test['pagesNumber']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>YMAA Publication Center</th>\n",
       "      <th>Yeoman House</th>\n",
       "      <th>Yesterday's Classics</th>\n",
       "      <th>Yoda Press</th>\n",
       "      <th>Zebra Historic Fantasy</th>\n",
       "      <th>Zebra Splendor</th>\n",
       "      <th>Zomba Books</th>\n",
       "      <th>dtv Deutscher Taschenbuchverlag</th>\n",
       "      <th>河出書房新社</th>\n",
       "      <th>pagesNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052262</td>\n",
       "      <td>-0.263308</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>0.128574</td>\n",
       "      <td>-0.161565</td>\n",
       "      <td>-0.127520</td>\n",
       "      <td>0.249588</td>\n",
       "      <td>0.037621</td>\n",
       "      <td>-0.074043</td>\n",
       "      <td>0.072854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.129112</td>\n",
       "      <td>0.021312</td>\n",
       "      <td>0.159166</td>\n",
       "      <td>-0.072448</td>\n",
       "      <td>0.036028</td>\n",
       "      <td>-0.093721</td>\n",
       "      <td>0.129199</td>\n",
       "      <td>0.069736</td>\n",
       "      <td>-0.253263</td>\n",
       "      <td>-0.066424</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170058</td>\n",
       "      <td>0.052351</td>\n",
       "      <td>-0.013406</td>\n",
       "      <td>0.099001</td>\n",
       "      <td>0.083173</td>\n",
       "      <td>-0.161439</td>\n",
       "      <td>0.048635</td>\n",
       "      <td>0.089419</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.063164</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250849</td>\n",
       "      <td>0.021555</td>\n",
       "      <td>0.091047</td>\n",
       "      <td>-0.041589</td>\n",
       "      <td>-0.040949</td>\n",
       "      <td>0.240260</td>\n",
       "      <td>0.415056</td>\n",
       "      <td>0.027029</td>\n",
       "      <td>-0.172413</td>\n",
       "      <td>-0.135485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.041681</td>\n",
       "      <td>0.038051</td>\n",
       "      <td>-0.051164</td>\n",
       "      <td>-0.076813</td>\n",
       "      <td>0.096855</td>\n",
       "      <td>-0.215943</td>\n",
       "      <td>0.152729</td>\n",
       "      <td>0.267636</td>\n",
       "      <td>-0.079954</td>\n",
       "      <td>-0.065560</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23058</th>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.019723</td>\n",
       "      <td>-0.003321</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>-0.129420</td>\n",
       "      <td>0.130302</td>\n",
       "      <td>-0.037361</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>-0.255112</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23059</th>\n",
       "      <td>-0.024484</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.015977</td>\n",
       "      <td>0.086630</td>\n",
       "      <td>0.082127</td>\n",
       "      <td>-0.174537</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.111608</td>\n",
       "      <td>-0.106961</td>\n",
       "      <td>-0.147956</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23060</th>\n",
       "      <td>-0.099309</td>\n",
       "      <td>-0.046230</td>\n",
       "      <td>-0.033294</td>\n",
       "      <td>0.242591</td>\n",
       "      <td>-0.055477</td>\n",
       "      <td>-0.033886</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>-0.126636</td>\n",
       "      <td>0.127742</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23061</th>\n",
       "      <td>-0.038388</td>\n",
       "      <td>0.065679</td>\n",
       "      <td>-0.159324</td>\n",
       "      <td>-0.048682</td>\n",
       "      <td>0.054175</td>\n",
       "      <td>0.317751</td>\n",
       "      <td>0.065931</td>\n",
       "      <td>-0.126021</td>\n",
       "      <td>-0.105057</td>\n",
       "      <td>-0.147185</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23062</th>\n",
       "      <td>-0.051934</td>\n",
       "      <td>-0.005339</td>\n",
       "      <td>0.030417</td>\n",
       "      <td>0.044278</td>\n",
       "      <td>-0.031339</td>\n",
       "      <td>-0.020412</td>\n",
       "      <td>0.038031</td>\n",
       "      <td>0.073363</td>\n",
       "      <td>-0.068429</td>\n",
       "      <td>-0.037205</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23063 rows × 5026 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.052262 -0.263308  0.026872  0.128574 -0.161565 -0.127520  0.249588   \n",
       "1     -0.129112  0.021312  0.159166 -0.072448  0.036028 -0.093721  0.129199   \n",
       "2     -0.170058  0.052351 -0.013406  0.099001  0.083173 -0.161439  0.048635   \n",
       "3      0.250849  0.021555  0.091047 -0.041589 -0.040949  0.240260  0.415056   \n",
       "4     -0.041681  0.038051 -0.051164 -0.076813  0.096855 -0.215943  0.152729   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23058  0.007497  0.000220  0.019723 -0.003321  0.021097 -0.129420  0.130302   \n",
       "23059 -0.024484  0.000467 -0.015977  0.086630  0.082127 -0.174537  0.011694   \n",
       "23060 -0.099309 -0.046230 -0.033294  0.242591 -0.055477 -0.033886  0.026869   \n",
       "23061 -0.038388  0.065679 -0.159324 -0.048682  0.054175  0.317751  0.065931   \n",
       "23062 -0.051934 -0.005339  0.030417  0.044278 -0.031339 -0.020412  0.038031   \n",
       "\n",
       "              7         8         9  ...  YMAA Publication Center  \\\n",
       "0      0.037621 -0.074043  0.072854  ...                        0   \n",
       "1      0.069736 -0.253263 -0.066424  ...                        0   \n",
       "2      0.089419 -0.072266 -0.063164  ...                        0   \n",
       "3      0.027029 -0.172413 -0.135485  ...                        0   \n",
       "4      0.267636 -0.079954 -0.065560  ...                        0   \n",
       "...         ...       ...       ...  ...                      ...   \n",
       "23058 -0.037361 -0.004281 -0.255112  ...                        0   \n",
       "23059  0.111608 -0.106961 -0.147956  ...                        0   \n",
       "23060  0.038410 -0.126636  0.127742  ...                        0   \n",
       "23061 -0.126021 -0.105057 -0.147185  ...                        0   \n",
       "23062  0.073363 -0.068429 -0.037205  ...                        0   \n",
       "\n",
       "       Yeoman House  Yesterday's Classics  Yoda Press  Zebra Historic Fantasy  \\\n",
       "0                 0                     0           0                       0   \n",
       "1                 0                     0           0                       0   \n",
       "2                 0                     0           0                       0   \n",
       "3                 0                     0           0                       0   \n",
       "4                 0                     0           0                       0   \n",
       "...             ...                   ...         ...                     ...   \n",
       "23058             0                     0           0                       0   \n",
       "23059             0                     0           0                       0   \n",
       "23060             0                     0           0                       0   \n",
       "23061             0                     0           0                       0   \n",
       "23062             0                     0           0                       0   \n",
       "\n",
       "       Zebra Splendor  Zomba Books  dtv Deutscher Taschenbuchverlag  河出書房新社  \\\n",
       "0                   0            0                                0       0   \n",
       "1                   0            0                                0       0   \n",
       "2                   0            0                                0       0   \n",
       "3                   0            0                                0       0   \n",
       "4                   0            0                                0       0   \n",
       "...               ...          ...                              ...     ...   \n",
       "23058               0            0                                0       0   \n",
       "23059               0            0                                0       0   \n",
       "23060               0            0                                0       0   \n",
       "23061               0            0                                0       0   \n",
       "23062               0            0                                0       0   \n",
       "\n",
       "       pagesNumber  \n",
       "0               48  \n",
       "1              364  \n",
       "2               32  \n",
       "3              293  \n",
       "4              352  \n",
       "...            ...  \n",
       "23058          120  \n",
       "23059           32  \n",
       "23060          132  \n",
       "23061          136  \n",
       "23062          192  \n",
       "\n",
       "[23063 rows x 5026 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([train_name_vec, train_authors_vec, train_desc_vec], axis=1)\n",
    "df_train.columns = range(len(df_train.columns))\n",
    "df_train = pd.concat([df_train, train_publisher], axis=1)\n",
    "df_train['pagesNumber'] = x_train['pagesNumber']\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23063, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we perform some feature selections on the doc2vec features\n",
    "mi = SelectKBest(mutual_info_classif, k=10)\n",
    "\n",
    "# first by finding some outstanding author features\n",
    "train_authors_mi = mi.fit_transform(train_authors_vec,y_train)\n",
    "test_authors_mi = mi.transform(test_authors_vec)\n",
    "\n",
    "train_authors_mi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Atheneum/Richard Jackson Books', 'Demented Dragon', 'EP BOOKS',\n",
       "       'Everymans Library', 'Gramercy', 'Hillsboro Press',\n",
       "       'Non-Duality Press', 'Simon Spotlight', 'Skylark Books',\n",
       "       'Tor Paranormal Romance'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get an idea on some of the publishers that determine the label\n",
    "# in a relatively larger extent\n",
    "mi.fit_transform(train_publisher, y_train)\n",
    "mi.get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Modelling and Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1: AutoGluon ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train)\n",
    "scaled_df = scaler.transform(df_train)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X_test, columns\u001b[39m=\u001b[39mscaled_df\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_df[\u001b[39m'\u001b[39m\u001b[39mrating_label\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m Y_test\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_df\u001b[39m.\u001b[39;49mto_csv(csv_file, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/generic.py:3563\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3552\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3554\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3555\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3556\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3561\u001b[0m )\n\u001b[0;32m-> 3563\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3564\u001b[0m     path_or_buf,\n\u001b[1;32m   3565\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[1;32m   3566\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3567\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3568\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3569\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3570\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3571\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3572\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3573\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3574\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3575\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3576\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3577\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3578\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3579\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3580\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1179\u001b[0m )\n\u001b[0;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:261\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[0;32m--> 261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:266\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    265\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_header()\n\u001b[0;32m--> 266\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_body()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:304\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m start_i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m end_i:\n\u001b[1;32m    303\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_chunk(start_i, end_i)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:315\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    312\u001b[0m data \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39miget_values(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(res\u001b[39m.\u001b[39mitems))]\n\u001b[1;32m    314\u001b[0m ix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_index[slicer]\u001b[39m.\u001b[39m_format_native_types(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n\u001b[0;32m--> 315\u001b[0m libwriters\u001b[39m.\u001b[39;49mwrite_csv_rows(\n\u001b[1;32m    316\u001b[0m     data,\n\u001b[1;32m    317\u001b[0m     ix,\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnlevels,\n\u001b[1;32m    319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcols,\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter,\n\u001b[1;32m    321\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_df, y_train, test_size=0.8, stratify=y_train)\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file = 'data.csv'\n",
    "# Convert the NumPy arrays to data frames\n",
    "train_df = pd.DataFrame(X_train, columns=scaled_df.columns)\n",
    "train_df['rating_label'] = Y_train\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns=scaled_df.columns)\n",
    "test_df['rating_label'] = Y_test\n",
    "train_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: data.csv | Columns = 5027 / 5027 | Rows = 11531 -> 11531\n",
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230517_174039/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (11531 samples, 463.73 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230517_174039/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.4\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 21.6.0: Thu Mar  9 20:08:59 PST 2023; root:xnu-8020.240.18.700.8~1/RELEASE_X86_64\n",
      "Train Data Rows:    11531\n",
      "Train Data Columns: 5026\n",
      "Label Column: rating_label\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == float, but few unique label-values observed and label-values can be converted to int).\n",
      "\t3 unique label values:  [3.0, 4.0, 5.0]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2865.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 463.64 MB (16.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 16.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2946 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1859): ['\"Kensington\"', '\"Pinnacle\"', '1st World Library', '2.13.61', '21 Publishing Ltd', '4th Estate, Limited', 'AA Publishing', 'AAA', 'ABDO & Daughters', 'ACC Distribution', 'ARCO', 'ASCD', 'ASTD', 'AVA', 'Aapc Publishing', 'Abacus (Little,Brown)', 'Acadian House Publishing', 'Accent Press (UK)', 'Ace Trade', 'Addison-Wesley', 'Adlard Coles', 'Adrenaline Audiobooks', 'Advisorpress', 'After the Battle', 'Aguilar', 'Akadine Press', 'Alabaster', 'Albin Michel', 'Alderac Entertainment Group', 'Alfaguara Infantil', 'Alfred A. Knopf Books for Young Readers', 'Alfred Publishing', 'Algonquin Books of Chapel Hill', 'Alianza Editorial, S. A.', 'Alias', 'Allison & Busby', 'Altamira Press', 'Altea', 'Amacom', 'Amadeus', 'Amber-Allen Publ., New World Library', 'Amereon House', 'America Star Books', 'American Atheist Press', 'American Express Food & Wine Magazine Corporation', 'Ammann', 'Anagrama', 'Andersen', 'Angel Island Association', 'Anker Publishing Company, Incorporated', 'Anthem Press', 'Apache Beach Publications', 'Apollos', 'Apple', 'Aquarium', 'Arbor House Publishing', 'Arbutus Press', 'Arcadia Books', 'Arcadia Publishing (SC)', 'Archipel', 'Archon Books', 'Arena', 'Ares Publishers', 'Ariadne Press', 'Ariadne Press (CA)', 'Aris & Phillips', 'Arkana/Penguin', 'Arkham House', 'Arms & Armour Press', 'Arnica Publishing', 'Arrow Books Ltd (Random House)', 'Arthaud', 'Artnik', 'Ashland Poetry Press', 'Aslan Publishing', 'Aspen Mlt, Inc.', 'Asphodel Press;', 'Assouline Publishing Corporation', 'Astrolog Publishing House', 'Atelos', 'Athol Books', 'Atlantean Press', 'Atlantic Books (NH)', 'Atlantic Coast Media', 'Atlantida', 'Atlas Press', 'Atrium Publishers Group', 'AudioGo', 'Aufbau-Verlag', 'Augsburg Books', 'Augsburg Fortress Publishers', 'Aunt Lute Books', 'Auschwitz-Birkenau State Museum', 'Autism Asperger Publishing Company', 'Autumn Pr', 'Aviation Supplies and Academics, Inc.', 'Aviv Press', 'Avon Inspire', 'Axios Press', 'B & W Publishing', 'B Fiction', 'BBC Physical Audio', 'BBC Publications', 'BDD Promotional Books Company', 'BOA Editions', 'Babel', 'Backbeat Books/United Entertainment Media, Inc.', 'Ballantine Books/Ivy Books', 'Bandai Entertainment', 'Bantam Bks.', 'Bantam Dell', 'Bantam Starfire', 'Bantam bks.', 'BantamSpectra', 'Baptist Healing Hospital Trust', 'Barnes & Noble Classics', 'Barnes Noble Classics', 'Barra Foundation', 'Barrington Stoke Ltd', 'Barrytown Limited', 'Basic Books, Inc. (NY)', 'Bassenian/Lagoni Architects', 'Bay Books & Tapes', 'Bay Books (CA)', 'Bayard Editions', 'Bdérogène', 'Beacon Publishing (OH)', 'Becker & Mayer', 'Benjamin Cummings', 'Bentley Publishers', 'Berkley Boooks', 'Berkley HEAT', 'Berkley Prime Crime', 'Berkley Prime Crime Penguin', 'Bethlehem Books', 'Bev Editions', 'Big Engine', 'Big Finish Productions', 'Big Finish Productions Limited', 'Big Red Chair Books', 'Billboard Books', 'Bindu Books', 'Birdsong Books', 'Bitter Lemon Press', 'Black & White Publishing', 'Black Flame', 'Black-Smith Enterprises', 'Blackstaff Press', 'Blackstone Audio, Inc.', 'Blackwell Publishing Professional', 'Bladud Books', 'Blake Publishing Ltd', 'Bleak House Books', 'Bloodaxe Books', \"Bloomsbury Children's Books\", 'Blue Dolphin Publishing', 'Blue Sky Press', 'BlueBridge', 'Bmj Publishing Group', 'Bog Walk Press', 'Bold Type Books', 'Bondage Books', 'Book Peddlers', 'Book Sales, Inc.', 'Bookhouse Fulfillment', 'Books for Libraries', 'Books for Midwives Press', 'Booksales', 'Bounty Books', 'Boxtree', 'Boys Town Press', 'Bradt Travel Guides', 'Bradygames', 'Bragelonne', 'Branden Publishing', 'Brandywine Press', 'Bravo', 'Breckling Press', 'Breese Books', 'Brett L Markham', 'Breur Media Corporation', 'Brick Books', 'Brigham Young University', 'Bristol Phoenix Press', 'Broadway Play Pub', 'Brookes Publishing', 'Brooklyn Botanic Garden', 'Browntrout Publishers', 'Bruce & Bruce', 'Brunton Company', 'Bulfinch Pr', 'Bull Publishing Company', 'Burke Publishing', 'CASSELL LBS (ORIO)', 'CLW Communications/AMG', 'CMX', 'CSA Word', 'Cadogan Guides', 'Caedmon', 'Cal Press', 'Calmann-Levy', 'Calvary Chapel Publishing', 'Campus Fachbuch', 'Canal House', 'Candle Books', 'Canongate Books Ltd', 'Canongate Us', 'Canterbury Press Norwich', 'Capra Press', 'Carmen Martinez Jover', \"Carnegie Endowment for Int'l Peace\", 'Carolrhoda Books', 'Casa Creacion', 'Casa Creación', 'Casablanca Press', 'Castle Books', 'Castle Hill Publishers', 'Caxton Editions', 'Caxton Press', 'Center Point Large Print', 'Centipede Press', 'Central Books', 'Century', 'Channel 4', 'Charles H. Kerr (Chicago)', 'Charles H. Kerr Publishers Company (Chicago)', 'Charles H. Kerr Publishing Co. (Chicago)', 'Charles River Media', \"Charles Scribner's Sons (NY)\", 'Charnwood', 'Chartwell Books', 'Chatterley Press International', 'Chelsea House Publishers', 'Cherry Red Books', 'Cheval Books', 'Chick Moorman', \"Child's Play International\", \"Children's Classics\", 'Childrens Press', 'Childrens Press Chicago', 'Chivalry Bookshelf', 'Chivers North America', 'Chivers P.', 'Choice Pub', 'Christian Bourgois', 'Christian Fellowship Publishers', 'Church Publishing', 'Cienfuegos', 'Cinebook Ltd', 'Circlet Press', 'City Lights Foundation Books', 'Classic Publishers', 'Clay Sanskrit', 'Cle', 'Clear Hot Media', 'Clear Light Publishing', 'Clear Water Press', 'Cliffs Notes', 'Cloudcap', 'Coach House Press', 'Collins  Brown', 'Collins & World', 'Collins Publishers', 'Colporteur', 'ComicsLit', 'Committee for UNICEF', 'Common Courage Press', 'Communio Books', 'Companhia das Letras', 'Companion Guides', 'Companion Press', 'ComteQ Communications', 'Conciliar Press', 'Connections Book Publishing', 'Constable', 'Constable & Robinson', 'Consumer Reports Books', 'Contemporary French Fiction', 'Conundrum Press (Canada)', \"Cook's Illustrated\", 'Corgi Childrens', 'Cork Hill Press', 'Cornell University - Cornell East Asia Series', 'Cornerstone Press Chicago', 'Cornerstone Publishing (Va)', 'Coscom Entertainment', 'Counterpoint LLC', 'Country Music Hall of Fame', 'Cowley Publications', 'Crabtree Publishing Company', 'Creative Arts Book Company', 'Crecy Publishing Ltd', 'Creflo Dollar Ministries', 'Crescent', 'Crescent Books', 'Crippen & Landru Publishers', 'Criterion', 'Crossway', 'Crowell', 'Crown House Publishing', 'Crown Publishers', 'Cumberland House', 'Currency Press Pty Ltd', 'Current Clinical Strategies Publishing', 'D.S. Brewer', 'DAW / SFBC', 'DK Publishing', 'DM Enterprises Incorporated', 'DNA Press', 'DTV', 'Daimon Verlag', 'Darby Creek Publishing', 'Darton,Longman & Todd Ltd', 'Davis', 'Daystar Productions', 'DeVorss & Company', 'Dearborn Real Estate Education', \"Debrett's Limited\", 'Dedalus', 'Dedalus Limited', 'Deeper Life Ministries, Inc', 'Del Rey/ballantine Books', 'Del Sol Publishing', 'Dell Young Yearling', 'Dembner Books', 'Denoël', 'Deodand Publishing', 'Deutscher Taschenbuch Verlag', 'Dewi Lewis Publishing', 'Dial Press Trade Paperback', 'Diana/Mexico', 'Digital Press', 'Diogenes Verlag', 'Dirty Danny Press', 'Discovery House', 'Disney Editions', 'Distribooks', 'Dog Ear Publishing', 'Dolphin Paperbacks', 'Donald I. Fine, Inc.', 'Donald M. Grant Publishers', 'Dorchester Publising Co.', 'Dork Storm Press', 'Doubleday & Company', 'Doubleday Childrens', 'Doubleday Religion', 'Dover', 'Dragon Door Publications', 'Dragonhawk Publishing', 'Drama Publishers/Quite Specific Media', 'Dreams Shared Publications', 'Drummond Publishing', 'Dtv', 'Duckworth Publishing', 'Dumont', 'Duquesne University Press', 'Dutton Adult', 'Dynamite Entertainment', 'E.P. Dutton & Co.', 'ESPN', 'ESRI Press', 'Eagle Gate Publishers', 'Earthscan Publications', 'Echelon Press Publishing', 'Eckankar', 'Eden Studios', 'Edgework Books', 'Ediciones Cátedra', 'Ediciones Giluz', 'Ediciones Granica, S.A.', 'Editions Dargaud', 'Editions Proveta', 'Editions des deux terres', 'Editionsié', 'Editora Globo', 'Editorial Diana, S.A.', 'Editorial Juventud SA', 'Editorial Pax Mexico', 'Editorial Seix Barral', 'Editorial Sirio', 'Editorial Tecnos', 'Editorial Trotta (Madrid)', 'Educators Press', 'Egmont', 'Egmont Ehapa', 'Egmont UK', 'Ekare', 'El Paso Norte Press', 'Eland Publishing', 'Elloras Cave', 'Emk Press', 'Emmaus Road Publishing', 'Emnin Books', 'Enchanted World', 'Endless Satsang Foundation', 'Engage Publishing', 'Entrepreneur Press', 'Equinox Publishing (UK)', 'Etruscan Press', 'Evan Moor Educational Publishers', 'Evans Brothers', 'Evergreen', 'Everyman Paperback', 'Everymans Library', 'Expert', 'FAB Press', 'Faber', 'Facts on File', 'Fairchild Books & Visuals', 'Fairchild Books AVA', 'Fairview Press', 'Faqir Publications', 'Farthest Star', 'Fawcett Crest', 'Fayard', 'Fearless Press', 'Federal Street Press', 'Feral House', 'Ferozsons (Pvt.) Ltd.', 'Fifth Estate', 'Fifth Estate Books', 'Filibust', 'Firebird', 'First', 'First Glance Books', 'Fisher', 'Five Leaves Publications', 'Fjord Press', 'Flame', 'Fleuve Noir', 'Flood Editions', 'Floris Books', 'Flower Valley Press', 'Foglight Press', 'Fondo de Cultura Económica', 'Foulsham', 'Foundation for a Course in Miracles', 'Fount Paperbacks', 'Four Winds Press', 'Fourth Dimension Publishing Company', 'Foxrock Books', 'Foyles', 'Fq Classics', \"Frances Lincoln Children's Books\", 'Franklin Publishers', 'Fraser Pub. Co.', 'Frederick Fell Publishers', 'Frederick Ungar', 'Frederick Warne and Company', 'Free Association Books', 'Freedom Press (CA)', 'Freedom Publishing Company (IL)', 'French & European Publications', 'Fresh Wind Press', 'Fretwater Press', 'Friedman/Fairfax Publishing', 'Friends of Israel Gospel Ministry', 'Funtreks Inc.', 'GIA Publications', 'GPP Travel', 'Gage Educational Publishing Company', 'Galahad Books, NY', 'Galaxy Press', 'Galaxy Press (CA)', 'Gallery / Saga Press', 'Gallimard Education', 'Gardner Press, Inc.', 'Garth Gardner Company', 'Gateway Editions', 'Gateway Learning Corporation', 'Gauntlet Press', \"Gay Men's Press\", 'Gemstone Publishing', 'Geneva Press', 'Gerald Duckworth & Company', 'Gestalten Verlag', 'Gestion 2000', 'Getty Publications', 'Ghost House Books', 'Gill Books', 'Gingko Press', 'Gladstone Publishing Limited', 'Gnomon Press', 'Go Comi', 'Goals Unlimited Press', 'Gold Leaf Press (WA)', \"Golden Guides from St. Martin's Press\", 'Golden/Disney', 'Good Earth Publications', 'Goose Lane Editions', 'Gospel Light Publications', 'Grafton', 'Gramercy Books/Random House Value Publishing, Inc.', 'Granary Books', 'Grand Central', 'Granta Books', 'Green Books', 'Green Key Books', 'Greenery Press', 'Greenwood Press, Inc.', 'Grijalbo', 'Groundworks', 'Grove Creek Publishing', 'Grove Press, Black Cat', 'Grupo Editorial Norma', 'Guggenheim Museum', 'Guild of Master Craftsman', 'HK Comics', 'Hachette Audio', 'Hachette Illustrated UK', 'Hackett Publishing Company Inc.', 'Half Halt Press', 'Hamish Hamilton', 'Hamlyn', 'Hampton Roads Publishing Company', 'Hanuman Foundation', 'Harcourt Brace College Publishers', 'Harcourt Brace Jovanich', 'Harcourt Brace and Company', \"Harcourt Children's Books\", 'Hard Shell Word Factory', 'Hardinge Simpole Limited', 'Harlan Davidson', 'Harlequin Author Spotlight', 'Harlequin Kimani New Spirit', 'Harlequin Nocturne', 'Harlequin SuperRomance # 986', 'Harper & Row (NY)', 'Harper & Row (NYC)', 'Harper Collins', 'Harper Collins Business', 'Harper Collins Trade Division', 'Harper Torchbooks', 'HarperCollins HarperTorch', 'HarperCollins Publishers Limited', 'HarperCollins Voyager', 'Harvard University Press (Cambridge)/Wm Heinemann (London)', 'Harvard University Press (Cambridge, MA)', 'Harvest Books', 'Hay House Audio Books', 'Haynes Publishing', 'Headline Book', 'Headline Feature', 'Healing Arts Press', 'Heartways Press', 'Henry Holt and Company', 'Herder & Herder', 'Herder, Freiburg', \"Here's Life Publishers\", 'Heyne', 'Hide This', 'Hill Street Press', 'Hillsboro Press', 'Hippocampus Press', 'Hisarlik Press', 'Hodder & Stoughton Ltd', 'Hodder Arnold', 'Hodder Education Publishers', 'Hodder Murray', 'Hodder Paperbacks', 'Hodder Stoughton', 'Holistic Life Enterprises', 'Holt, Rinehart and Winston, Inc.', 'Home Farm Books', 'Hors Collection', 'Houghton Mifflin Harcourt (HMH)', 'House of Collectibles', 'House of Nehesi', 'Hovel Audio', 'Howard Fertig', 'Huber, Bern', 'Humanoids Publishing', 'Hundreds of Heads Books', 'Hungry Minds', 'Huntington House Publishers', 'Hyperion Press', 'I.B. Tauris', 'IBM Press', 'IDG Books Worldwide, Inc.', 'Icon Books Ltd', 'Iifso', 'Immanion Press/Magalithica Books', 'Incommunicado Press', 'Independent Music Press', 'Influence Press (Deborah King)', 'Inkwater Press', 'Inner City Books', 'Inner Ocean/Innisfree Press', 'Inspirational Press (NY)', 'Institute for Management Development, Incorporated', 'Institute of Physics Publishing (GB)', 'Intellect Ltd', 'Intercultural Press', 'Intrepid Traveler', 'Intrigue Press', 'Irvington Pub', 'Islamic Texts Society', 'Italica Press', 'Ivan R. Dee', 'Ivy Books, Ballantine Books', 'J. Countryman', 'J.A. Allen & Company, Limited', 'JAI Press', 'JIST Works', 'Jade Mountain Publishing', 'Jaico Publishing House', 'Jeremy P. Tarcher', 'Jist Publishing', \"John Gray's Mars Venus LLC\", 'Jon Carpenter Publishing', 'Jonathan Cape (London)', 'Judaica Press', 'Juno Books', 'Juventud', 'K & S Ranch', 'K Publications', 'Kalmbach Media', 'Kamera Books', 'Kaveri Books,India', 'Kehot Publication Society', 'King Hell Press', 'Kiplinger Books', 'Kirkbride Bible Company', 'Kiseido Publishing Company', 'Klett', 'Klett-Cotta', 'Knaur', 'Knight', 'Knopf Publishing Group', 'Kokinos', 'Kqed Books', 'Krieger Publishing Company', 'Kuperard', 'Kyle Books', 'Kyle Cathie', 'L.A. Weekly Books', 'LSU Press', 'La Fabrique Editions', 'La Plata, Buenos Aires: Terramar Ediciones', 'Ladybird Books Ltd', 'Lantern Australia', 'Lantern Books', 'Lara Adrian, LLC', 'Large Print Press', 'Lark Crafts', 'Larousse Bilingual/French', 'Larousse Editions', 'Larousse Mexico', 'Latin American Literary Review Press', 'Laurence King Publishing', 'Lawrence & Wishart', 'Le livre de poche', \"Leete'S Island Books\", 'Legacy Words', 'Legal Awareness Series', 'Leo Cooper', 'Lerner Classroom', 'Librairie des Champs-Elysees', 'Libros Tigrillo', 'Life of Reiley', 'Lifeway Church Resources', 'Light Technology Publications', 'Lightyear Press', 'Lily Hill Publishing', 'Linkgua', 'Linnet Books', 'Lionheart Publishing', 'Little Books', 'Little Brown & Co (P)', 'Little Brown & Co (T)', 'Little Imp', 'Little Tiger Press', 'Little, Brown & Company', 'Littman Library of Jewish Civilization', 'Liturgy Training Publications', 'Livres Toundra', 'Llanerch Publishers', 'Llewellyn Espanol', 'Lodestar Books', 'Loeb Classical Library 192', 'Loewe', 'Loewe Verlag', 'London: Jonathan Cape', 'Lonely Planet Publications', 'Longman Higher Education', 'Losada', 'Love Inspired', 'Love Spell Books', 'Luath Press Ltd', 'LucasBooks', 'Lucis Press', 'Lumeneditorial', 'Lutterworth Press', 'Lyle Stuart', 'M Press', 'M. Evans & Co.', 'MFA Publications', 'MITCH', 'MR Ediciones', 'MacAdam Cage', 'MacAdam/Cage', 'MacHillock Publishing', 'MacMillan', 'MacMillan UK', 'Macmillan Australia', 'Macmillan U.K.', 'Macmillan UK', 'Macroprintbooks', 'Madwoman Press', 'Magabala Books', 'Magination Press', 'Magna Large Print Books', 'Mainstream Publishing Company', 'Mandarin', 'Mansell', 'Mantra Lingua', 'March/Abrazo Press', 'Marquette University Press', \"Marshall Cavendish Children's Books\", 'Marsu productions', 'Martínez Roca', 'Masquerade Books', 'Massachusetts Museum of Contemporary Art', 'Mayo Clinic', 'McArthur & Co / Headline', 'McGraw Hill Higher Education', 'McGraw-Hill College', 'McGraw-Hill Education / Medical', 'McGraw-Hill Education Tab', 'McGraw-Hill Osborne Media', 'Me+mi Publishing', 'Mediasat Group', 'Megan Tingley Books', 'Mel Bay Publications', 'Mel Bay Publications, Inc.', 'Melville House', 'Mercer University Press', 'Mercury Press', 'Meredith Corporation', 'Mesa House Publishing', 'Mestas Ediciones', 'Methuen Drama', 'Methuen Publishing Ltd; New edition edition', 'Metro Books', 'Metro Publishing', 'MetroBooks', 'Metropolitan Museum of Art New York', 'Meyer & Meyer Sport', 'Michael Di Capua Books', 'Michael Glazier', 'Michel Lafon', 'Michigan State University Press', 'Milady Publishing', 'Milan', 'Millbrook Press', 'Millbrook Press (Tm)', \"Miller's Publications\", 'Mindfield Publications', 'Minedition', 'Minx', 'Mission Bay Press', \"Mister Beller's Neighborhood\", 'Mitchell Lane Publishers', 'Monash Asia Institute', 'Mondo Publishing', 'Mongoose Publishing', 'Monkfish Book Publishing', 'Morgan Reynolds Publishing', 'Motley Fool', 'Mott Media (MI)', 'Multnomah Gifts', 'Multnomah Publishers Inc.', 'Music Sales', 'Mystic Seaport Museum', 'NAL Hardcover', 'NTC/Contemporary Publishing Company', 'NY Fawcett Crest 1991.', 'NY Review of Books Classics', 'NY Times Book Co. (NYC)', \"NYR Children's Collection\", 'Naiad Press', 'Nan A. Talese', 'Nation Books (NYC)', 'National Journal Group', 'National Science Teachers Association', 'New Amsterdam Books', 'New City Press', 'New Dawn Press(IL)', 'New Internationalist', 'New Line Books', 'New Trends Publishing', 'New York Review Books', 'New York Review of Books', 'New in Chess', 'Newnes', 'Nicotext', 'Night Shade', 'Nitty Gritty Cookbooks', 'No Exit', 'No Nonsense Fly Fishing Guidebooks', 'No Starch Press', 'Noguer Y Caralt', 'Nolo', 'Nolo Press Occidental', 'Noontide Press', 'Norfolk Press', 'Norilana Books', 'Norma S A Editorial', 'Norman Hall', 'North South', 'North-Holland', 'NorthSouth (NY)', 'Northern Illinois University Press', 'Northfield Publishing', 'Northland Publishing', 'Northword Press', 'Novoice Unheard', 'Numen', 'O Books', \"O'Reilly Media, Inc.\", 'O. W. Barth Bei Scherz', 'ONEWorld Publications', 'OUP UK', 'Obelisco', 'Ocean Press (AU)', 'Ocean Sur', 'Old New York Book Shop Press', 'Olive Branch Press', 'Olympia Press', 'Omega Publications', 'Omnigraphics', 'One World/Ballantine Books/Random House, Inc.', 'Onyx/Penguin Books (NY)', 'Orion Publishing Group, Ltd.', 'Orion mass market paperback', 'Other Press (Asia)', 'Our Sunday Visitor (IN)', 'Oxford University Press (NYC)', 'P. Zsolnay', 'P.D. Publishing, Inc.', 'Pacer Books', 'Pacific Presss Publishing Association', 'Paidos Iberica Ediciones S a', 'Paidotribo Editorial', 'Palazzo', 'Pan Australia', 'Pan Books (UK)', 'Pan Publishing', 'Panorama Mexico', 'Pantheon (NY)', 'Pantheon Books', 'Paradise Cay Publications', 'Paradox Press', 'Parenting Press', 'Parents Magazine Press', 'Park Street Press', 'Parker Hudson', 'Parkstone Press', 'Parkway Publishers', 'Pathbinder Publishing, LLC', 'Patrice Michelle', 'Pauline Books & Media', 'Pavilion', 'Pavilion Books', 'Payot / Rivages', 'Peachpit Press', 'Peanut Butter and Jelly Press', 'Pearson Ed Asia', 'Pelican Publishing Company', 'Pemberley Press', 'Penguin Adult Hc/Tr', 'Penguin Australia', 'Penguin Books & Pearson Education ESL', 'Penguin Books Australia Ltd', 'Penguin Books Limited', 'Penguin Canada', 'Penguin Group (Australia)', 'Penguin Press HC, The', 'Penguin/Pearson ESL', 'Penmarin Books', 'Penton Kids', 'Península', 'Persephone Books Ltd', 'Perseus Books', 'Peter Lang Gmbh, Internationaler Verlag Der Wissenschaften', 'Peter Lang Inc.', 'Peter Owen Ltd', 'Peter Owen Ltd.', 'Peter Pauper Press', 'Peter Weed Books', 'Pfeiffer & Company', 'Phantasia Press', 'Philippe Picquier', 'Phoenix Books/University of Chicago Press (IL)', 'Phoenix Publishing Inc', 'Piccadilly Books', 'Piccolo', 'Picnic Point Press', 'Pierides Press', 'Pinata Books', 'Pine Street Books', 'Pineapple Press', 'Piper', 'Piper Verlag', 'Pitkin Pictorials Limited', 'Pittsburgh Filmmakers', 'Players Publishing', 'Plaza & Janes Editores, S.A.', 'Plough Publishing House', 'Pocket Books (NY)', 'Pocket Books, New York', 'Pocket Books/Simon & Schuster, Inc.', 'Pogo Press', 'Point', 'Pomona Press', 'Poolbeg Press', 'Popular Library', 'Popular Prakashan Ltd', 'Popular Press 1', 'Port Hole Publications', 'Portable Press', 'Poseidon Press', 'Post Apollo Press', 'Potes & Poets Press', 'PowerKids Press', 'PracticeSpot Press', 'Prana', 'Prentice Hall College Div', 'Press Publishing', 'Prestígio', 'Priddy Books Us', 'Princeton University Press (NY)', 'Prion (GB)', 'Pro-Ed', 'Profile Books(GB)', 'Prufrock Press', 'Publications International, Ltd.', 'Purdue University Press', 'Purple Bear Books', 'Purple Haze Press Llc', 'Purple House Press', 'Pushkin Press', 'Putnam', 'Quadrillion Publishing', 'Quality Medical Publishing', 'Quill Driver Books', 'Quinteto', 'Quiver Books', 'R & L Education', 'R.M. Barry Publishing', 'RAND Corporation', 'RH Audio Dimensions', 'RIC Publications', 'Racquet Tech Publishing', 'Radcliffe Publishing', 'Ragged Bears USA', 'Rainbow Books', 'Rainbow Studies International', 'Rand McNally', 'Random House Canada', 'Random House Inc.', 'Random House Trade', 'Ravensburger', 'Re/Search Publications', \"Reader's Digest Young Families\", 'Readers Digest', 'Realms', 'Rebel Pub. Co.', 'Reclam, Ditzingen', 'Recorded Books, Inc.', 'Recovery Publications', 'Red Deer Press', 'Red Hen Press', 'Red Wagon Books', 'Red Wheel', 'Redbone Press', 'Regal Crest Enterprises - Quest', 'Reiman Media Group', 'Renew', 'Research  Education Association', 'Research & Education Association', 'Reverie Publishing', 'Reward Books', 'Rider', 'Rip Squeak Press', 'Rising Moon Books', 'Rising Star Press', 'Rita Gerlach', 'Rivergate Books', 'Riverhead Books/The Berkley Publishing Group/Penguin Putnam Inc.', 'Riverrun Press (New York, NY)', 'Rob Weisbach Books', 'Robert Hale', 'Robert Kennedy Publishing', 'Roberts and Company Publishers', 'Robinson', 'Robson Books', 'Roc Fantasy', 'Rock Spring Press', 'Rolling Homes Press', 'Ronin Publishing', 'Ronin Publishing (CA)', 'Rose Publishing (CA)', 'Rosen Central', 'Rosicrucian Fellowship', 'RotoVision', 'Routledge & Kegan Paul (London)', 'Routledge Publishers', 'Routledge,Taylor & Francis Books Ltd imprint', 'Rowman & Littlefield Education', 'Rowohlt', 'Rowohlt TB-V., Rnb.', 'Royal Academy Books', 'Royal Ontario Museum', 'Rubery Press', 'Rudra', 'Rupa Publications', 'SAF Publishing Ltd', 'SIMON & SCHUSTER CHI', 'SOM Publishing', 'ST Publishing', 'Safari Press', 'Sage Publications', 'Saint Joseph Communications', 'San Francisco Museum of Modern Art', 'Santa Monica Press', 'Sarabande Books', 'Saturnalia Books', 'Savas Beatie', 'Scalo Publishers', 'Schirmer G Books', 'Schoenhof Foreign Books', 'Scholars Press', 'Scholastic Point', 'School for Advanced Research Press', 'Scientia Verlag', 'Screech Owls', 'Scribble  Sons', 'Scrub Jay Journeys', 'Sea Bird Publishing', 'Sea Change Press', 'Seagull Books', 'Seahorse Press', 'Secker & Warburg', 'Second Story Press', 'Seedsowers', 'Seishinsha Co., Ltd.', 'Semiotext(e)', 'Seuil (Points)', 'Seven Dials', 'Sheep Meadow Press', 'Sheffield Academic Pr', 'Shelter Publications', 'Sidgwick & Jackson Ltd', 'Sierra Club Books', 'Siglo XXI Ediciones', 'Signet Classic', 'Sil International, Global Publishing', \"Silhouette By Request 2's\", 'Silhouette Montana Mavericks', 'Silkworm Books', 'Siloam', 'Silverwood Press', 'Simon  Schuster UK', 'Simon & Schuster (NYC)', 'Simon & Schuster (Trade Division)', 'Simon & Schuster Adult Publishing Group', 'Simon & Schuster Australia', 'Simon & Schuster Inc.', 'Sinauer Associates Is an Imprint of Oxford University Press', 'Sirius Entertainment', 'Sixth & Spring Books', 'Skira', 'Skoob Books (GB)', 'Sky Books (NY)', 'Skylark Books', 'Skyscape', 'Slab-O-Concrete Publications', 'Slack Incorporated', 'Small Talk Publishing', 'Smart Art Press', 'Smashwords Edition', 'Smithsonian', 'Smithsonian Institution Press', 'Smyth & Helwys Publishing', 'Snow Lion Publications', 'Society for Promoting Christian Knowledge', 'Solid Ground Christian Books', 'Sophia Institute Press', 'Sort of Books', 'Southern Methodist University Press', 'Southwater Publishing', 'Souvenir Press', 'Souvenir Press Ltd', 'Special Interest Model Books', 'Speck Press', 'Spellmount', 'Spinifex Press', 'Sport Media Publishing', 'Spotlight', 'Springboard Press', 'Squall Press', 'St Bedes Publishing', 'St Martins Press', 'St. Augustines Hardwood Press', \"St. Paul's House\", 'Standard Publishing Company', 'Steck-Vaughn', 'Steerforth Italia', 'Sterling Publishing (NY)', 'Sterling Publishing Co. Inc. (NY)', 'Still Eye Rise Media, LLC', 'Stillpoint Press, Inc.', 'Stone Arch Books', 'Stonewall Inn Mysteries', 'Storey Publishing', 'Story Line Press', 'Struik Publishers', 'Subira Publishing', 'Sudamerica-Planeta', 'Summersdale', 'Summit Books', 'Summit University Press', 'Sun & Moon', 'Sun and Moon Press', 'Sunset Productions', 'Sunstone Press', 'Supercollege, Llc', 'Sutton', 'Sutton Publishing Ltd', 'Swallowdown Press', 'Swan Isle Press', 'Syngress Publishing', 'Syracuse University Press (NY)', 'Syracuse University Publications in Continuing Education', 'T. & T. Clark Publishers', 'TELOS', 'TFH Publications', 'TSR, Inc.', 'Talisman House, Publishers', 'Talonbooks', 'Tarascon Publishing', 'Target Books, Universal Publishing & Distributing Corporati', 'Tatra Press', 'Taunton', 'Tauris Parke Paperbacks', 'Taylor Productions Ltd.', 'Taylor Publishing Company (TX)', 'Telos Publishing Ltd', 'Ten Speed Pr', 'Tender Buttons Press', 'Terramar Ediciones', 'Teton New Media', 'Th1nk Books', 'Tharpa Publications', 'The Blue Sky Press', 'The Book Foundation', 'The Church of Jesus Christ of Latter-day Saints', 'The Doubleday Religious Publishing Group', 'The Greenwich Workshop Press', 'The Macmillan Co.,', 'The Penguin Press', 'The Taunton Press, Inc.', 'The Well-Trained Mind Press', 'Theosophical University PR', 'Theses & Dissertatons Pr', 'Theytus Books', 'Thieme', 'Think Books', 'Third World Press', 'Thirsty Books', 'Thoemmes', 'Thomas Allen Publishers', 'Thomson PDR', 'Thomson West', 'Thorogood', 'Threshold Editions', 'Thunder Bay Press (CA)', \"Thunder's Mouth Press (NY)\", \"Thunder's Mouth Press (NYC)\", 'Tibor de Nagy Editions', 'Tiger Books / Senate', 'Tilbury House Publishers', 'Timber Press (OR)', 'Time Life', 'Tomato Enterprises', 'Tommy Nelson', 'Tor', 'Torkids', 'Touchstone (Simon & Schuster) New York', 'Touchstone/Simon & Schuster', 'Touring Club of Italy', 'Tradeselect', 'Trafalgar Square', 'Trans-Atlantic Pubns', 'Transworld Distribution Services, Incorporated', 'Trinity Press International', 'Trumpet Club', 'Tsaba House', 'Tsunami Books', 'Tudor Publishing', 'Turtle Point Press', 'Twayne Publishers', 'Twelfth Night Press', 'Twin Palms Publishers', 'Twisted Spoon Press', 'Two Lions', 'Tyndale Entertainment', 'Tyndale House Publishers, Inc.', 'U.S. Government Printing Office', \"UBS Publishers's Distributors\", 'USCCB', 'Uitgeverij De Arbeiderspers', 'Unidad Editorial', 'Unique Publications', 'Unity House', 'Univ Tennessee Press', 'University Of Hertfordshire Press', 'University Press of America', 'University Science Books', 'University of Alaska Press', 'University of British Columbia Press', 'University of Illinois Press (Urbana)', 'University of Michigan Library', 'University of Michigan, Taubman College of Archite', 'University of Nebraska Press (Bison Book)', 'University of Rochester Press', 'University of Utah Press', 'University of the West Indies Press', 'Upper Access', 'VIZ Media, LLC', 'Valentine Publishing House', 'Valley Press', 'Vanguard Productions', 'Vanguard Productions (NJ)', 'Vega', 'VeloPress', 'Verba Mundi', 'Vergara', 'Vertigo (DC Comics)', 'Vestal Press', 'Victor', 'Vintage Espanol', 'Virago Press', 'Virgin', 'Vision Forum', 'Vital Books', 'Voice', 'W.W. Norton & Co Inc', 'W.W. Norton & Co Inc. (NY/London)', 'W.W. Norton & Company (London/NY)', 'WS Publishing Group', 'WW Norton', 'WWW.Readhowyouwant.com', 'Waking Lion Press', 'Walk Worthy Press', 'Wallflower', 'Walter Foster Publishing', 'Watson-Guptill Publications', 'Weidenfeld Military', 'Wenner Books', 'West Publishing Company', 'West St. James Press', 'White Cloud Press', 'White Pine Press', 'White Wolf Studio', 'Whitlands Publishing', 'Whitston Publishing Company', 'Whole Health Books', 'Wildcat Canyon Press', 'Wilder Publications', 'Wildstorm', 'William Andrew', 'William S. Konecky Associates', 'Willow Creek Press', 'Willowtree Press', 'Wilshire Book Company', 'Windhorse Publications (UK)', 'WingSpan Press', 'Wings', 'Wm. B. Eerdmans Publishing Co.', \"Woman's Missionary Union\", \"Women's Press (UK)\", 'Woodland Publishing', 'Word for Today', 'Wordsworth Classics', 'World Bank Publications', 'Writer', 'Writers & Readers Publishing', 'Wynn Publishing Company', 'Yellow Moon Press', 'Yen Press', 'York Medieval Press', 'Young Lions', 'Zarahemla Books', 'Zebra Ballad', 'Zeig, Tucker & Theisen', 'Zoland Books', 'Zzdap Publishing', 'debolsillo', 'توپ', 'هرمس', '5 Spot', 'A Shannon Ravenel Book', 'A.R.E. Press (Association of Research & Enlightenment)', 'Abdo Publishing Company', 'Abrams', 'Actes Sud', 'Adell Press', 'Advanced Vivarium Systems', 'Agir', 'Aladdin Classics', 'Aladdin/Minstrel Book', 'Alfred A Knopff, Inc.', 'Alive Books', 'Allia', 'Allison and Busby', 'Alma Books', 'AmazonEncore', 'Ambrosia Publications', 'American Chemical Society', 'American Diabetes Association', 'American Master Products, Inc.', 'Amsco Publications', 'Anadem Publishing', 'Anchor Bible', 'Annies', 'Ansel Adams', 'Anvil Press', 'Apple Press', 'Arbor House Publishing Company (NY)', 'Arcade Books', 'Arkham House Publishers', 'Army War College Foundation Press', 'Arno Press', 'Association for Supervision & Curriculum Development', 'Astiberri', 'Atheneum/Anne Schwartz Books', 'Atlantic Publishers & Distributors (P) Ltd.', 'Atlantic Publishing Company (FL)', 'Atlas Press (GB)', 'Axel Menges', 'BBC Worldwide', 'BET', 'BT Press', 'Backcountry Guides', 'BainBridgeBooks (PA)', 'Bambú', 'Barrytown/Station Hill Press, Inc.', 'Basil Blackwell Publishers (Oxford)', 'Bastei Lübbe', 'Bay Tree Publishing', 'Bayfield Street Publishing', 'Beaver Books', 'Ben Uri Gallery & Museum', 'Berkley (NYC)', 'Berkley Medallion', 'Berkley Publishing', 'Better Baby Press', 'Bibliopola Press', 'Billboard Books/Watson-Guptill Publications', 'Black (Aus)', 'Black Industries', 'Black Velvet Seductions Forbidden Experiences', 'Blackstone Audio, Inc', 'Blackwell Science', 'Blake', 'Blooms Literary Criticism', 'Bloomsbury Publishing Limited', 'Bloomsbury Publishing, PLC', 'Bodleian Library, University of Oxford', 'Bonnier Carlsen', 'Books Britain', 'Books on Tape', 'Bramble Books', 'Bridge Publications, Inc.', 'Brigham Young University Press', 'Brighter Child', 'Brookings Institution Press and AEI', 'Brooks Publishing Company', 'Brown Thrasher Books', 'Bucknell University Press', 'CCH Incorporated', 'CF4kids', 'CMC Publishing', 'Caledonia Press', 'Cambrdge University Press', 'Can of Worms Press', 'Capital Books (VA)', 'Capstone Publishing', 'Cardoza Publishing', 'Carlsen', 'Cassell PLC', 'Center for American Places', 'Center for Big Bend Studies', 'Ceres Press', 'Charis Books', 'Chartwell Books, Inc.', 'Chatto and Windus', 'Cherry Lane Music Company', 'Chiasmus Press', 'Childs World', 'Chivers Large print (Chivers, Windsor, Paragon & C', 'Chivers Word for Word Audio Books', 'Cisne', 'Citadel: Kensington and Open Road Media', 'Civitas Books', \"Claitor's Pub Division\", 'Collins Press', 'Combined Publishing', 'Compact Classics', 'Copy Workshop', 'Coronet Books (GB)', 'Cottonwood Press (Fort Collins, CO)', 'Council for Research in Values and Philosophy', 'Creative Bound', 'Creative Company', 'Crown Archetype', 'Crown Trade Paperbacks', 'Crowood Press (UK)', 'Cruise Books', 'Crystal Clarity Publishers', 'Crítica', 'Dandelion Enterprises', 'Davies-Black Publishing', 'Dawson Publishing', 'De Arbeiderspers', 'Debate Editorial', 'Delta Books/Dell Publishing Co., Inc.', 'Derrydale', 'Desert Publications', 'Destino Ediciones', 'Dharma Pub', 'Dillon Press', 'Disinformation Books', 'Disney', 'Dispatch Travels', 'Dodd Mead', 'Dogwise Publishing', 'Dorset House', 'Doubleday & Company, Inc.', 'Dover Publications, Inc.', 'Dragon Moon Press', 'Drama Book Publishing', 'Dramaline Publications', 'Dramaqueen', 'Droemersche Verlagsanstalt Th. Knaur Nachf., GmbH & Co.', 'Dufour Editions', 'E.P. Dutton', 'ESPN Books', 'Earthpulse Press', 'Eastern Washington University Press', 'Edge Press LLC', 'Ediciones Barataria', 'Edition Stemmle', 'Editorial Anagrama', 'Editorial Diana', 'Editorial Planeta', 'Editorial Porrúa, S.A.', 'Eland Publishing Ltd', 'Elixir Press', 'Elva Resa Publishing', 'Enchanted Lion Books', 'Epicenter Press', 'Ermor Enterprises', 'Exact Change', 'Expert Books', 'Fairwinds Press', 'Family Health Publications(CA)', 'Family Life Publishing', 'Fantasy Flight Games', 'Far Eastern Press', 'Figures', 'First Coast Books', 'Fischer', 'Fitzhenry & Whiteside Limited', 'Fontana Books', 'Food First Books', 'Food N Sport Press', 'Footprint Handbooks', 'Foundation Press', 'FourthEstate', \"Frances Lincoln Children's Bks\", 'Frank Cass Publishers', 'Franklin Square Press', 'Frederick Muller', 'Frog in Well', 'Frontline', 'Full Cast Audio', 'G.K. Hall Large Print nonfiction series', 'G.T. Labs', 'Galahad', 'Gallimard / Folio', 'Gallimard Jeunesse', 'Gallup Press', 'Garland Pub.', 'Gedisa', 'Gerstenberg, Hildesh.', 'Global Publishing', 'Goal/QPC', 'Gomer Press', 'Good Apple', 'Gospel Light', 'Grace & Truth Books', 'Gray & Company Publishers', 'Great Plains Teen Fiction', 'Green Ronin Publishing', 'Greenhaven Press', 'Greenleaf Publications (Tuscaloosa)', 'Grove Press (NY)', 'Grupo Anaya Comercial', 'HNL', 'Handsel Books', 'Hannibal Books', 'Hansen House Publishing', 'Hanuman Books', 'Harcourt Brace Jovanovich', 'Harcourt Brace Jovanovich, Publishers', 'Harlequin Flipside', 'Harlequin Kimani Sepia', 'Harlequin SuperRomance #1088', 'Harley Books', 'Harper & Row, Publishers', 'Harper Collins Publishers', 'Harper San Francisco', 'Harper Thorsons', 'HarperCollins Publisher', 'HarperLuxe', 'Haus Publishers Ltd.', 'Healing Society', 'Health Text Audio / McGraw-Hill', 'Heinemann Educational Books - Library Division', 'Heritage House Publishers', 'Himalayan Institute Press', 'Home Arts', 'Hopeace Press', 'Howells House', 'Hugh Lauter Levin Associates', 'Humanoids - Rebellion', 'Huron River Press', 'Hurst & Co.', 'Hutchinson Radius', 'ILR Press', 'ISIS Publishing', 'Ian Allan Publishing', 'Impact', 'Imprint Academic', 'IndyPublish', 'Inse', 'Instant Improvement', 'International Universities Press', 'Islamic Foundation', 'J.-C. Lattès', 'J.M. Dent & Sons', 'Jacana Media', 'Japan Publications Trading', 'John Curley & Associates', 'John Fielder Publishing', 'John Libbey & Company', 'Jonathan David Publishers', 'Jones Books', 'José Corti', 'Julliard', 'Jupitalia Productions', 'Kaplan Business', \"Kate's Mystery Books\", 'Kensington Publishing', 'KidHaven Press', 'Konemann', 'L. Cooper', 'LGF', 'Larousse', 'Last Unicorn Games, Inc.', 'Lattes', 'Laurence Holt Books', 'Le Félin', 'Left Coast Press', \"Let's Go Publications\", 'Librio', 'Life Journey', 'Lighthouse Review Inc', 'Ligonier Ministries', 'Linbrook Press', 'Listening Chamber', 'Literary Express, Inc', 'Litmus Press', 'Little Brown & Company', 'Little, Brown Spark', 'Loeb Classical Library', 'Lone Eagle Publishing Company', 'Looseleaf Law Publications', 'Lorimer', 'Los Angeles County Museum of Art', 'Lotus Press', 'Lotus Press (WI)', 'Love Inspired Classics', 'Loyola Classics', 'LucasBooks for Young Readers', 'Lucent Books', 'Ludwig Von Mises, Institute', 'Lyford Books', 'Lynx Books', 'Lyrick Publishing', 'MBI', 'Madison Books', 'Madison Park Press', 'Maeva', 'Magvető Könyvkiadó', 'Mandala Publishing (CA)', 'Margaret Weis Productions', 'Marion Street Press, Inc.', 'Marvel Knights', 'Maupin House Publishing', 'Meadow Books', 'Megan Tingley', 'Meisha Merlin Publishing', 'Memento Mori Mystery', 'Mercier Press', 'Meridian', 'Metro Books,London', 'Microscope Publications Ltd.', 'Millenium', 'Miramax', 'Miramax/Hyperion', 'Missile Rider Publishing', 'Missouri History Museum Press', 'Mnémos', 'Modern Publishing', 'Modern Times', 'Monash University Publishing', 'Monkeysuit Press', 'Morpheus International', 'Morrow Junior Books', 'Moyer Bell and its subsidiaries', 'Museum of New Mexico Press', 'Mystery Company', 'Naked Ink', 'Narrative Press', 'NeWest Press', 'Nelson Thornes', 'Network Theory.', 'New American Library', 'New York:  Vanguard Press', 'NewSouth Books', 'Newcastle Publishing Company', 'Newgrail Media', 'Nexus', 'Night Sky (North South Books)', 'Nijgh & Van Ditmar', 'Nimbus Publishing (CN)', 'No Greater Joy Ministries', 'Noonday Press', 'Nord-Süd-Verlag', 'North Atlantic Books, Frog Ltd.', 'Northeast Foundation for Children', 'Norton & Company', 'OGO Books', 'Octagon Press, Limited', 'Odyssey Books  Maps', 'On Cape Publications', 'One World (UK)', 'Onlywomen Press', 'Ontario Review Press', 'Orchard Books', \"Orion Children's Books (an Imprint of The Orion Publishing Group Ltd )\", 'Orion London', 'Our Daily Bread Publishing', 'Owl Books', 'Owlet Paperbacks', 'Ozark Mountain Publishing', 'P E M a Publications, Incorporated', 'Palisades', 'Palm Press', 'Pan Books Limited', 'Pantheon Books (NY)', 'Parachute Press (NY)', 'Paradigm Publications (MA)', 'Parigramme', 'Paul Dry Books', 'Paul Norbury Global Books Ltd. (UK)', 'Pearson Education', 'Pearson Education ESL', 'Pen and Sword', 'Penguin Group (USA)', 'Personhood Press', 'Peter Russell', 'Phanes Press', 'Phoenix Press', 'Phoenix Press (CA)', 'Phoenix Press (UK)', 'Phébus', 'Piggy Toes Press', 'Pinon Press', 'Piper Taschenbuch', 'Plum Creek Press', 'Pogue Press', 'Polebridge Press', 'Policy Press', 'Praxis Publications Inc', 'Pritchett & Hull Associates, Incorporated', 'Promise Press', 'Public Square Books', 'Puffin Canada', 'Pure Life Ministries', 'R.L. Crow Publications', 'Racom Communications', 'Ramsay', 'Random House Espanol', 'Random House Inc (T)', 'Random House Puzzles & Games', 'Random House of Canada Ltd', 'Razorbill', \"Reader's Digest Children's Books\", 'Reclam Verlag GmbH', 'Republic of Texas Press', 'Request Audiobooks', 'Research Associates School Times Publications', 'Reynolds  Hearn', 'Richard Kasak Book', 'Ringpress Books', 'Riverhead', 'Rizzoli Intl Pubns', 'Robert Reed Publishers', 'Roc Trade', 'Rodale, Inc.', 'Roever & Associates', 'Rotovision', 'Rough Guides Limited', 'Roxbury Publishing Company', 'Rugged Land Books', 'Russell Meerdink Company', 'SITES', 'SaltRiver', 'Samuel French Ltd, London', 'Sandra Cabot', 'Saunders Ltd.', 'Savage Studios', 'Savas Publishing', 'Schwartz & Wade', 'Scion Publishing', 'Scribner Book Company SL117', 'Scripta Mercaturae', 'Seabury Books', 'Secker  Warburg', 'Secret Passage Press', 'Sentinel HC', 'Serres', 'Sevenstar Communications', 'Shasta Abbey Press', 'Sheed & Ward', 'Sierra Club Books For Children', 'Sierra Club Books for Children', 'Siglo XXI', 'Signal Books', 'Signet Books (NY)', 'Silman-James Press', 'Simon', 'Soundprints', 'Sourcebooks Jabberwocky', 'Southbank Publishing', 'Spellmount Publishers', 'Spence Publishing Company', 'Spinsters Ink', 'Spinsters Ink Books', 'Sports Illustrated', 'Spruce', \"St. Martin's\", \"St. Martin's Minotaur/Thomas Dunne\", 'Stacey International Publishers', 'Starbooks', 'Stationery Office Books (TSO)', 'Sterling Publ. Co. Inc', 'Stoddart', 'Stonewall Inn Editions', 'Story Press Books', 'Streams Publishing Co', 'Summer Bridge Activities', 'Swan Hill Press', 'Sword & Sorcery Studio', 'TFH Publications, Inc.', 'Taller del Exito', 'Tarquin Group', 'Terrail', 'The American Reprint Company', 'The Berkley Publishing Group', 'The Chicken House', 'The Disinformation Company (St. Paul, MN)', 'The Golden Sufi Center', 'The Johns Hopkins University Press / Woodrow Wilson Center Press', 'The Modern Library', 'The Noonday Press; Farrar, Straus and Giroux', 'The Random House', 'Theia', 'Third Millennium', \"Thomas Dunne Books (St. Martin's Press)\", 'Thomas T. Beeler Publisher', 'Thorndike Pr (Largeprint)', 'Thunder Bay Press (MI)', 'Tiger Tales', 'Time-Life Books', 'Time-Life Books (Alexandria, VA)', 'Totally Unique Thoughts', 'Touchstone/Simon & Schuster (NY)', 'Trailer Life Publications', 'Transaction Publishers', 'Transworld Publishers, London', 'Trine Day', 'Trinity University Press', 'Tropical R', 'True 2 Life Productions', 'Turtleback Books/Demco Media distribution', 'Twilight Times Books (TN)', 'U.S.: Sterling (Italy: Gruppo Editoriale Armenia)', 'UNESCO', 'Ueberreuter', 'United States Institute of Peace Press', 'University of Michigan Press/Regional', 'University of Queensland Press', 'Urban Soul', 'Urj Press', 'Utah State University Press', 'Verse Chorus Press', 'Viceroy Press', 'Vintage Books/Random House, Inc.', 'WLC', 'Walker Childrens Paperbacks', 'Walker,', 'Wallbuilders Inc', 'Ward Lock Educational Co Ltd', 'Warde Publishers', 'Warner Books Inc.', 'Wayfarer Publishing (GB)', 'Welcome Books', 'West Point Military Series', 'West Virginia University Press', 'WestBow Press', 'Western Publishing Company, Inc./Golden Books', 'Westminster John Knox Press, (Louisville, KY)', 'Westview Pr (Short Disc)', 'Westview Publishing', 'White Star', 'Whitehorse Press', 'WildStorm', 'Wimmer Cookbooks', 'Wind in the Pines Publishing', 'Women Unlimited', 'Woodbine House', 'Woodstream Publishing', 'Writers Digest Books', 'YA Angst', 'YMAA Publication Center', 'Yeoman House', \"Yesterday's Classics\", 'Yoda Press', 'Zebra Historic Fantasy', 'Zebra Splendor', 'Zomba Books', 'dtv Deutscher Taschenbuchverlag', '河出書房新社']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 3167 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     :  221 | ['0', '1', '2', '3', '4', ...]\n",
      "\t\t('int', ['bool']) : 2946 | ['\"Puffin\"', '100 Minute Bible', '1st Impression Publishing', '1st World Library - Literary Society', '2.13.61 Publications', ...]\n",
      "\t355.1s = Fit runtime\n",
      "\t3167 features in original data used to generate 3167 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 54.36 MB (2.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 357.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 10377, Val Rows: 1154\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\tWarning: Model is expected to require 18.63% of available memory... (20.0% is the max safe size.)\n",
      "\t0.6412\t = Validation score   (accuracy)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\tWarning: Model is expected to require 18.75% of available memory... (20.0% is the max safe size.)\n",
      "\t0.649\t = Validation score   (accuracy)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.6386\t = Validation score   (accuracy)\n",
      "\t69.43s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: '/usr/local/opt/libomp/lib/libomp.dylib'\n",
      "  Referenced from: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so'\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\n",
      "Fitting model: LightGBM ...\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: '/usr/local/opt/libomp/lib/libomp.dylib'\n",
      "  Referenced from: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so'\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7028\t = Validation score   (accuracy)\n",
      "\t467.07s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7028\t = Validation score   (accuracy)\n",
      "\t410.84s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\tMany features detected (2999), dynamically setting 'colsample_bylevel' to 0.33344448149383127 to speed up training (Default = 1).\n",
      "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
      "\t0.7028\t = Validation score   (accuracy)\n",
      "\t34.55s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.7045\t = Validation score   (accuracy)\n",
      "\t35.77s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.7028\t = Validation score   (accuracy)\n",
      "\t28.08s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.7088\t = Validation score   (accuracy)\n",
      "\t129.84s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7088\t = Validation score   (accuracy)\n",
      "\t46.83s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: '/usr/local/opt/libomp/lib/libomp.dylib'\n",
      "  Referenced from: '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightgbm/lib_lightgbm.so'\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7132\t = Validation score   (accuracy)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1598.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230517_174039/\")\n"
     ]
    }
   ],
   "source": [
    "# Create a TabularDataset from the CSV file\n",
    "dataset = TabularDataset('data.csv')\n",
    "label = 'rating_label'\n",
    "predictor = TabularPredictor(label=label).fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   WeightedEnsemble_L2   0.713172       1.538221  748.846123                0.001342           0.423423            2       True         11\n",
      "1        NeuralNetTorch   0.708839       0.131760   46.826022                0.131760          46.826022            1       True         10\n",
      "2               XGBoost   0.708839       0.167171  129.844858                0.167171         129.844858            1       True          9\n",
      "3        ExtraTreesGini   0.704506       0.232029   35.765547                0.232029          35.765547            1       True          7\n",
      "4              CatBoost   0.702773       0.195391   34.549130                0.195391          34.549130            1       True          6\n",
      "5      RandomForestEntr   0.702773       0.200178  410.844580                0.200178         410.844580            1       True          5\n",
      "6        ExtraTreesEntr   0.702773       0.212924   28.081638                0.212924          28.081638            1       True          8\n",
      "7      RandomForestGini   0.702773       0.218934  467.071289                0.218934         467.071289            1       True          4\n",
      "8        KNeighborsDist   0.649047       0.595077    0.703636                0.595077           0.703636            1       True          2\n",
      "9        KNeighborsUnif   0.641248       0.618290    0.699357                0.618290           0.699357            1       True          1\n",
      "10      NeuralNetFastAI   0.638648       0.228546   69.427765                0.228546          69.427765            1       True          3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.713172</td>\n",
       "      <td>1.538221</td>\n",
       "      <td>748.846123</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.423423</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuralNetTorch</td>\n",
       "      <td>0.708839</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>46.826022</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>46.826022</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.708839</td>\n",
       "      <td>0.167171</td>\n",
       "      <td>129.844858</td>\n",
       "      <td>0.167171</td>\n",
       "      <td>129.844858</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTreesGini</td>\n",
       "      <td>0.704506</td>\n",
       "      <td>0.232029</td>\n",
       "      <td>35.765547</td>\n",
       "      <td>0.232029</td>\n",
       "      <td>35.765547</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.702773</td>\n",
       "      <td>0.195391</td>\n",
       "      <td>34.549130</td>\n",
       "      <td>0.195391</td>\n",
       "      <td>34.549130</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestEntr</td>\n",
       "      <td>0.702773</td>\n",
       "      <td>0.200178</td>\n",
       "      <td>410.844580</td>\n",
       "      <td>0.200178</td>\n",
       "      <td>410.844580</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTreesEntr</td>\n",
       "      <td>0.702773</td>\n",
       "      <td>0.212924</td>\n",
       "      <td>28.081638</td>\n",
       "      <td>0.212924</td>\n",
       "      <td>28.081638</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestGini</td>\n",
       "      <td>0.702773</td>\n",
       "      <td>0.218934</td>\n",
       "      <td>467.071289</td>\n",
       "      <td>0.218934</td>\n",
       "      <td>467.071289</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>0.649047</td>\n",
       "      <td>0.595077</td>\n",
       "      <td>0.703636</td>\n",
       "      <td>0.595077</td>\n",
       "      <td>0.703636</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>0.641248</td>\n",
       "      <td>0.618290</td>\n",
       "      <td>0.699357</td>\n",
       "      <td>0.618290</td>\n",
       "      <td>0.699357</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>0.638648</td>\n",
       "      <td>0.228546</td>\n",
       "      <td>69.427765</td>\n",
       "      <td>0.228546</td>\n",
       "      <td>69.427765</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  score_val  pred_time_val    fit_time  \\\n",
       "0   WeightedEnsemble_L2   0.713172       1.538221  748.846123   \n",
       "1        NeuralNetTorch   0.708839       0.131760   46.826022   \n",
       "2               XGBoost   0.708839       0.167171  129.844858   \n",
       "3        ExtraTreesGini   0.704506       0.232029   35.765547   \n",
       "4              CatBoost   0.702773       0.195391   34.549130   \n",
       "5      RandomForestEntr   0.702773       0.200178  410.844580   \n",
       "6        ExtraTreesEntr   0.702773       0.212924   28.081638   \n",
       "7      RandomForestGini   0.702773       0.218934  467.071289   \n",
       "8        KNeighborsDist   0.649047       0.595077    0.703636   \n",
       "9        KNeighborsUnif   0.641248       0.618290    0.699357   \n",
       "10      NeuralNetFastAI   0.638648       0.228546   69.427765   \n",
       "\n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                 0.001342           0.423423            2       True   \n",
       "1                 0.131760          46.826022            1       True   \n",
       "2                 0.167171         129.844858            1       True   \n",
       "3                 0.232029          35.765547            1       True   \n",
       "4                 0.195391          34.549130            1       True   \n",
       "5                 0.200178         410.844580            1       True   \n",
       "6                 0.212924          28.081638            1       True   \n",
       "7                 0.218934         467.071289            1       True   \n",
       "8                 0.595077           0.703636            1       True   \n",
       "9                 0.618290           0.699357            1       True   \n",
       "10                0.228546          69.427765            1       True   \n",
       "\n",
       "    fit_order  \n",
       "0          11  \n",
       "1          10  \n",
       "2           9  \n",
       "3           7  \n",
       "4           6  \n",
       "5           5  \n",
       "6           8  \n",
       "7           4  \n",
       "8           2  \n",
       "9           1  \n",
       "10          3  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: test.csv | Columns = 5027 / 5027 | Rows = 11532 -> 11532\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'test.csv'\n",
    "df_test.to_csv(csv_file, index=False)\n",
    "test_data = TabularDataset('test.csv')\n",
    "preds = predictor.predict(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Use BayesSearchCV to find the best n_neighbors value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m bs_search \u001b[39m=\u001b[39m BayesSearchCV(knn, param_grid, n_iter\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m bs_search\u001b[39m.\u001b[39;49mfit(X_train, Y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Print the best parameters and accuracy score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest n_neighbors value:\u001b[39m\u001b[39m\"\u001b[39m, bs_search\u001b[39m.\u001b[39mbest_params_[\u001b[39m'\u001b[39m\u001b[39mn_neighbors\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skopt/searchcv.py:466\u001b[0m, in \u001b[0;36mBayesSearchCV.fit\u001b[0;34m(self, X, y, groups, callback, **fit_params)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_kwargs_ \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_kwargs)\n\u001b[0;32m--> 466\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my, groups\u001b[39m=\u001b[39;49mgroups, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    468\u001b[0m \u001b[39m# BaseSearchCV never ranked train scores,\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m# but apparently we used to ship this (back-compat)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_train_score:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skopt/searchcv.py:512\u001b[0m, in \u001b[0;36mBayesSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39mwhile\u001b[39;00m n_iter \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# when n_iter < n_points points left for evaluation\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     n_points_adjusted \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_iter, n_points)\n\u001b[0;32m--> 512\u001b[0m     optim_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(\n\u001b[1;32m    513\u001b[0m         search_space, optimizer,\n\u001b[1;32m    514\u001b[0m         evaluate_candidates, n_points\u001b[39m=\u001b[39;49mn_points_adjusted\n\u001b[1;32m    515\u001b[0m     )\n\u001b[1;32m    516\u001b[0m     n_iter \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n_points\n\u001b[1;32m    518\u001b[0m     \u001b[39mif\u001b[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skopt/searchcv.py:408\u001b[0m, in \u001b[0;36mBayesSearchCV._step\u001b[0;34m(self, search_space, optimizer, evaluate_candidates, n_points)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# make lists into dictionaries\u001b[39;00m\n\u001b[1;32m    406\u001b[0m params_dict \u001b[39m=\u001b[39m [point_asdict(search_space, p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params]\n\u001b[0;32m--> 408\u001b[0m all_results \u001b[39m=\u001b[39m evaluate_candidates(params_dict)\n\u001b[1;32m    409\u001b[0m \u001b[39m# Feed the point and objective value back into optimizer\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m# Optimizer minimizes objective, hence provide negative score\u001b[39;00m\n\u001b[1;32m    411\u001b[0m local_results \u001b[39m=\u001b[39m all_results[\u001b[39m\"\u001b[39m\u001b[39mmean_test_score\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(params):]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:708\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    705\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mfit_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    707\u001b[0m fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m--> 708\u001b[0m test_scores \u001b[39m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[1;32m    709\u001b[0m score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:767\u001b[0m, in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[1;32m    765\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test)\n\u001b[1;32m    766\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 767\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[1;32m    768\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m error_score \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:429\u001b[0m, in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_passthrough_scorer\u001b[39m(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    428\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Function that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39;49mscore(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 666\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neighbors/_classification.py:219\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m    Class labels for each data sample.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muniform\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    217\u001b[0m     \u001b[39m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X, return_distance\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    220\u001b[0m     neigh_dist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/neighbors/_base.py:763\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    756\u001b[0m use_pairwise_distances_reductions \u001b[39m=\u001b[39m (\n\u001b[1;32m    757\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbrute\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    758\u001b[0m     \u001b[39mand\u001b[39;00m PairwiseDistancesArgKmin\u001b[39m.\u001b[39mis_usable_for(\n\u001b[1;32m    759\u001b[0m         X \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    761\u001b[0m )\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[0;32m--> 763\u001b[0m     results \u001b[39m=\u001b[39m PairwiseDistancesArgKmin\u001b[39m.\u001b[39;49mcompute(\n\u001b[1;32m    764\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    765\u001b[0m         Y\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[1;32m    766\u001b[0m         k\u001b[39m=\u001b[39;49mn_neighbors,\n\u001b[1;32m    767\u001b[0m         metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[1;32m    768\u001b[0m         metric_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_params_,\n\u001b[1;32m    769\u001b[0m         strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    770\u001b[0m         return_distance\u001b[39m=\u001b[39;49mreturn_distance,\n\u001b[1;32m    771\u001b[0m     )\n\u001b[1;32m    773\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    774\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbrute\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m issparse(X)\n\u001b[1;32m    775\u001b[0m ):\n\u001b[1;32m    776\u001b[0m     results \u001b[39m=\u001b[39m _kneighbors_from_graph(\n\u001b[1;32m    777\u001b[0m         X, n_neighbors\u001b[39m=\u001b[39mn_neighbors, return_distance\u001b[39m=\u001b[39mreturn_distance\n\u001b[1;32m    778\u001b[0m     )\n",
      "File \u001b[0;32msklearn/metrics/_pairwise_distances_reduction.pyx:691\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction.PairwiseDistancesArgKmin.compute\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/threadpoolctl.py:171\u001b[0m, in \u001b[0;36m_ThreadpoolLimiter.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mtype\u001b[39m, value, traceback):\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestore_original_limits()\n\u001b[1;32m    174\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39mcls\u001b[39m, controller, \u001b[39m*\u001b[39m, limits\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, user_api\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Bayess earch to tune the hyperparameters for knn\n",
    "skf_CV = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 10)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train)\n",
    "scaled_df = scaler.transform(df_train)\n",
    "for train_idx, test_idx in skf_CV.split(scaled_df, y_train):\n",
    "    # train-test split\n",
    "    X_train, X_test = scaled_df[train_idx], scaled_df[test_idx]\n",
    "    Y_train, Y_test = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "    param_grid = {'n_neighbors': (1, 50),\n",
    "                  'weights': ['distance', 'uniform']}\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # Use BayesSearchCV to find the best n_neighbors value\n",
    "    bs_search = BayesSearchCV(knn, param_grid, n_iter=20, cv=5)\n",
    "    bs_search.fit(X_train, Y_train)\n",
    "\n",
    "    # Print the best parameters and accuracy score\n",
    "    print(\"Best n_neighbors value:\", bs_search.best_params_['n_neighbors'])\n",
    "    print(\"Best weight:\", bs_search.best_params_['weights'])\n",
    "    print(\"Accuracy:\", bs_search.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7069152395404292\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7106004769130717\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071320182094082\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.705767562879445\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEWCAYAAAAEkA60AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoaklEQVR4nO3dd7xcVbn/8c/3nPSQkAqEhBIgRCFKuZEi5YYeigb9gQQREFGMEgUFuaBcURTEiqCUGwEFQu8R6U1AKQmRXgNJSO+FNHLOyfP7Y+8TJoeZOZMwc2bOnO/79dqvzKy9Zu9nhvBkrb32XksRgZmZfVxNuQMwM6tUTpBmZjk4QZqZ5eAEaWaWgxOkmVkOTpBmZjk4QbZRkjpL+rukJZJu+wTHOU7SQ8WMrRwk3S/pxHLHYZXFCbLCSfqqpAmSlkmalf6PvHcRDn0UsCnQOyKO3tCDRMQNEXFwEeJZh6RhkkLSXU3Kd0rLnyjwOD+TNLa5ehFxaERcu4HhWpVygqxgkn4I/BG4kCSZbQlcDowowuG3At6OiPoiHKtU5gF7SuqdUXYi8HaxTqCE/z+w7CLCWwVuwMbAMuDoPHU6kiTQmen2R6Bjum8YMB04A5gLzAJOSvf9HFgN1KXnOBn4GTA249hbAwG0S99/HXgP+ACYDByXUf50xuc+D4wHlqR/fj5j3xPAL4B/pcd5COiT47s1xn8lcGpaVgvMAH4KPJFR9xJgGrAUeAHYJy0f3uR7vpQRxwVpHCuB7dKyb6b7rwDuyDj+r4FHAZX774W3lt38L2fl2hPoBNyVp85PgD2AnYGdgN2AczP2b0aSaPuTJMHLJPWMiPNIWqW3RMRGEXF1vkAkdQUuBQ6NiG4kSfDFLPV6Af9I6/YG/gD8o0kL8KvAScAmQAfgzHznBq4DTkhfHwK8SvKPQabxJL9BL+BG4DZJnSLigSbfc6eMzxwPnAJ0A6Y2Od4ZwGckfV3SPiS/3YkR4edy2xgnyMrVG5gf+bvAxwHnR8TciJhH0jI8PmN/Xbq/LiLuI2lFDd7AeNYAQyR1johZEfFaljqHA+9ExPURUR8RNwFvAl/IqPPXiHg7IlYCt5Iktpwi4t9AL0mDSRLldVnqjI2IBek5f0/Ssm7ue/4tIl5LP1PX5HgrSH7HPwBjge9FxPRmjmdVyAmyci0A+khql6fO5qzb+pmalq09RpMEuwLYaH0DiYjlwDHAKGCWpH9I+lQB8TTG1D/j/ewNiOd6YDSwH1la1JLOlPRGOiK/mKTV3KeZY07LtzMiniO5pCCSRG5tkBNk5XoG+BA4Mk+dmSSDLY225OPdz0ItB7pkvN8sc2dEPBgRBwH9SFqFfykgnsaYZmxgTI2uB74L3Je27tZKu8BnAV8BekZED5Lrn2oMPccx83aXJZ1K0hKdmR7f2iAnyAoVEUtIBiMuk3SkpC6S2ks6VNJv0mo3AedK6iupT1q/2VtacngR2FfSlpI2Bs5p3CFpU0kj0muRH5J01ddkOcZ9wPbprUntJB0D7ADcu4ExARARk4H/Jrnm2lQ3oJ5kxLudpJ8C3TP2zwG2Xp+RaknbA78EvkbS1T5L0s4bFr21Zk6QFSy9nvZDkoGXeSTdwtHA3WmVXwITgJeBV4CJadmGnOth4Jb0WC+wblKrSeOYCSwkSVbfyXKMBcARJIMcC0haXkdExPwNianJsZ+OiGyt4weBB0hu/ZkKrGLd7nPjTfALJE1s7jzpJY2xwK8j4qWIeAf4MXC9pI6f5DtY6yMPzJmZZecWpJlZDk6QZmY5OEGameXgBGlmlkO+m5ArUgd1ik7qWu4wKlZ9ny7NV2rj2s1bXu4QKt4HLJofEX039POH7Nc1FixsKKjuCy9/+GBEDN/Qc5VSq0uQndSVPToeWu4wKtbco3ctdwgVr+8Vz5Q7hIr3SNze9Imo9TJ/YQPPPTigoLrt+73b3FNPZdPqEqSZtQZBQ2R7lqB1cYI0s6ILYE3+pzlbBSdIMyuJNVmfRm1dnCDNrOiCoM5dbDOzjwugwV1sM7PsfA3SzCyLABqqYCIcJ0gzK4nWfwXSjxqaWQkEQUOBW3MkdZL0vKSXJL0m6edp+UBJz0maJOkWSR3S8o7p+0np/q0zjnVOWv6WpEOaO7cTpJkVXQTUFbgV4ENg/3RVyp2B4ZL2IFmO9+KI2A5YRLL6JOmfi9Lyi9N6SNoBGAnsSLIk8OWSavOd2AnSzEpANBS4NScSy9K37dMtgP2B29Pya/lo/aYR6XvS/QdIUlp+c0R8mC7jMYlkqeScnCDNrOgCWBOFbSSrd07I2E5pejxJtZJeBOYCDwPvAoszVu2czkerZ/YnXXYj3b+EZBnlteVZPpOVB2nMrCQKaR2m5kfE0HwVIqIB2FlSD5Klf7MtO1x0bkGaWdElN4oXp4u9znEjFgOPA3sCPTLWjR/AR8sLzwC2gLWLsG1Msojc2vIsn8nKCdLMii6AuqgpaGtOuqxxj/R1Z+Ag4A2SRHlUWu1E4J709bj0Pen+xyJZnXAcMDId5R4IDAKez3dud7HNrOgC0VC89lc/4Np0xLkGuDUi7pX0OnCzpF8C/wGuTutfTbJM7ySSZYpHAkTEa5JuBV4nWUv91LTrnpMTpJmVxJpYv+5zLhHxMrBLlvL3yDIKHRGrgKNzHOsC4IJCz+0EaWZF13gNsrVzgjSzEhANBVxfrHROkGZWdMmM4k6QZmYfEyFWR96n+FoFJ0gzK4k1vgZpZvZxySCNu9hmZll4kMbMLCsP0piZ5dFQpBvFy8kJ0syKLhB10frTS+v/BmZWcTxIY2aWQyB3sc3McvEgTRvUtVs9p/96Mltvv5IIuPisgaxaVcv3fzmZTl3WMGdGR35z+rasWNb6nyLI5bwvPM4+g6aycHlnvvJ/xwBw4Kff5dv/PYGBfRZx/NVf5o1Zm6ytf9JeEzly5zdpCPHbB/bmmfc+mrO0RmsY+807mLe0K6fdcliLf5dyGzpsKaN+MZPamuD+m3px6583LXdIRRFBVdzmU7JvkGupxiZ1ci7PWKlGnTeVF/65Md868LN897AhvD+pMz/41WSu+c0WfOfQz/DvB3ty1Cmzyh1mSf39pcGMvvHwdcrendeLM287hIlT+61TPrDPQg7Z8V2OuvIYRt94OGcf+hQ1+mjF5GN3e4XJ83u2SNyVpqYmOPXCGZx73EC+NWww+41YzJaDVpU7rKJIBmlqC9oqWSlTfK6lGjNlXZ6xUnXpVs9ndvuAB27pC0B9XQ3LP2hH/4GreOW5bgBMfLo7ew1fWM4wS27i+5uzZGXHdcomz+/J1AU9PlZ32OApPPjattQ11DJzcXemL+rOkM3nArBJt2XsM+h97v7Pp1si7IozeJcVzJzSgdnvd6S+roYn7unBnocsKXdYRdNATUFbJStZdHmWasyUa3nGirTZgA9ZsrA9Z/x2Mn++91VOv2gyHTs3MPWdzux50GIA9j1sIX37rS5voBVkk27LmbN0o7Xv5yzdiL7dlwNw5iH/5pJH9mhc2a7N6b1ZHfNmdlj7fv6s9vTpV1fGiIonEGuisK2SlTR9N12qMSKea1Il1/KMFam2XbDdjsu594ZNGH3EEFatqOGY78ziD2cN5Ijj5/Cnca/Suesa6usq+z96JUiuYXbijdl9yx2KlUg1tCBLOkjTdKlGSUMi4tX1PU66Tu4pAJ3oUtwg18P8WR2YP7sDb72YtIieur8Xx4yayXV/GMBPTkhWoew/cCW77b+4bDFWmrkfdGXT7svWvt+0+zLmLe3KvoOn8t/bT2Xv7cbSoV0DXTvW8csjH+Xcuw8oY7Qta8Hs9vTd/KPeRp9+dcyf1b6MERVPsi52ZSe/QrTIN8hYqnF4k125lmds+vkxETE0Ioa2V6cSR5vbovkdmDerAwO2WQnALp9fwvuTOrNx76RbJAXHjp7JP27YJN9h2pR/vr01h+z4Lu1rG9i8x1K26LWEV2duwp8f251DLzmeI/70Nc6580AmTN68TSVHgLde7EL/gavZdIsPadd+DcNGLObZhzYud1hFUtiSr5W+LEPJWpCS+gJ1EbE4Y6nGpoMwjcszPsO6yzNWrMvP24qzLn6X9h2CWe935A8/2oYDvjyfL5wwB4B/PdCLh27rU+YoS+vCLz3Cf201kx5dVnH/addz5T+HsnRlJ84a/jQ9u6zk0pH38/ac3px64xG8N68XD7++DbePuoWGEBfdv09VtCyKYU2DuOwn/bnwxveoqYWHbu7F1LfL1wAopmTZ18oeoS6ESpWPJH2WZAAmc6nG8yWdD0yIiHGSOgHXk6xYthAYma5UllP3mt6xR8dDSxJzNZj7jV3LHULF63vFM+UOoeI9Ere/EBFDN/Tz/XfsEd+9de+C6p475B+f6FylVLIWZJ6lGn+a8Trn8oxm1rpVw43ifpLGzIoumQ+ysq8vFsIJ0sxKoDpmFG/938DMKk5ym09xbhSXtIWkxyW9nj62fFpa/jNJMyS9mG6HZXzmnPQR5rckHZJRPjwtmyTp7ObO7RakmRVd47PYRVIPnBEREyV1A16Q9HC67+KI+F1mZUk7ACOBHYHNgUckbZ/uvozkjprpwHhJ4yLi9VwndoI0s5Io1nRnETELmJW+/kDSGyRP4eUyArg5Ij4EJkuaBOyW7pvUeKeMpJvTujkTpLvYZlZ0yXRnKmgD+kiakLGdkuu46YxfuwCNjy2PlvSypGskNU4LtfYR5tT0tCxXeU5uQZpZSazHRBTzC7kPUtJGwB3A6RGxVNIVwC9ILnn+Avg98I0NDDcrJ0gzK7pkNp/idVAltSdJjjdExJ0AETEnY/9fgHvTt2sfYU4NSMvIU56Vu9hmVnTJo4Y1BW3NSadAvBp4IyL+kFGeOTvzl4DGiXDGASPTCbkHAoOA54HxwCBJAyV1IBnIGZfv3G5BmlkJFLUFuRdwPPBKOn0iwI+BYyXtTJKPpwDfBoiI1yTdSjL4Ug+cms4shqTRwIMkj0BfExGv5TuxE6SZlUSxnqSJiKch68Huy/OZC4ALspTfl+9zTTlBmlnRNY5it3ZOkGZWEtUwrZ0TpJkVXeOaNK2dE6SZFV0A9W5Bmpll5y62mVk2rWBJ10I4QZpZ0XnCXDOzPNyCNDPLonHC3NbOCdLMii4Q9Ws8SGNmlpWvQZqZZRPuYpuZZeVrkGZmeThBmpllEYgGD9KYmWXnQRozsyzCgzRmZrmFE6SZWTaerMLMLCe3IMtlTZQ7goo18X+vKHcIFe+QK3YudwhVLwIa1jhBmpll5VFsM7MsAnexzcxy8CCNmVlOUQVDBa3/WSAzq0gRKmhrjqQtJD0u6XVJr0k6LS3vJelhSe+kf/ZMyyXpUkmTJL0sadeMY52Y1n9H0onNndsJ0syKLhnFriloK0A9cEZE7ADsAZwqaQfgbODRiBgEPJq+BzgUGJRupwBXQJJQgfOA3YHdgPMak2ouTpBmVhIRhW3NHydmRcTE9PUHwBtAf2AEcG1a7VrgyPT1COC6SDwL9JDUDzgEeDgiFkbEIuBhYHi+c/sapJmVxHqMYveRNCHj/ZiIGJOtoqStgV2A54BNI2JWums2sGn6uj8wLeNj09OyXOU5OUGaWdEFhV1fTM2PiKHNVZK0EXAHcHpELJU+On5EhKSiDwu5i21mJREFboWQ1J4kOd4QEXemxXPSrjPpn3PT8hnAFhkfH5CW5SrPyQnSzIovINaooK05SpqKVwNvRMQfMnaNAxpHok8E7skoPyEdzd4DWJJ2xR8EDpbUMx2cOTgty8ldbDMriSI+SbMXcDzwiqQX07IfAxcBt0o6GZgKfCXddx9wGDAJWAGclMQTCyX9Ahif1js/IhbmO7ETpJmVRLFuFI+IpyHng90HZKkfwKk5jnUNcE2h586ZICX9iTyXCCLi+4WexMzalrbwLPaEPPvMzHILoJoTZERcm/leUpeIWFH6kMysGrSJZ7El7SnpdeDN9P1Oki4veWRm1ooVNoJdyCh2ORVym88fSR7RWQAQES8B+5YwJjOrBsW8EbJMChrFjohpmXetAw2lCcfMqkJU/yBNo2mSPg9Eejf7aSQPi5uZ5VbhrcNCFNLFHkVyT1F/YCawMznuMTIz+4gK3CpXsy3IiJgPHNcCsZhZNVlT7gA+uUJGsbeR9HdJ8yTNlXSPpG1aIjgza6Ua74MsZKtghXSxbwRuBfoBmwO3ATeVMigza/2KNWFuORWSILtExPURUZ9uY4FOpQ7MzFq5ar7NJ12/AeB+SWcDN5N8nWNIZsswM8utwrvPhcg3SPMCSUJs/JbfztgXwDmlCsrMWr/iz+/d8vI9iz2wJQMxsyoSggp/jLAQBT1JI2kIsAMZ1x4j4rpSBWVmVaCaW5CNJJ0HDCNJkPeRrDn7NOAEaWa5VUGCLGQU+yiSWXtnR8RJwE7AxiWNysxav2oexc6wMiLWSKqX1J1k5bAtmvtQtbr26ZdYsbyWNQ3Q0CC+/4UdOefPkxiwzSoANurewLKltZx62JAyR1o6q1eJM768HXWra2ioh30OX8IJP5rNPdf04a6r+jJrSkdufeUVNu6dzGmyfGkNvx69FXNndqChHo4aNY9DRi7k3Vc786dzBrD8gxpqa2Hk9+cwbMTi8n65FtR389X86JL36dG3HgLuG9ubu6/uW+6wiqPaJ8zNMEFSD+AvJCPby4BnCj2BpFqS2clnRMQRTfZ1JOmq/xfJdGrHRMSUQo9dLv8zcjBLF7Vf+/5Xo7db+/pb577P8qW15QirxbTvGPzmtnfp3HUN9XXwwyMH8bn9l7Lj55az+0FLOev/bbdO/XF/68OW26/i/Osms3hBLSfv82n2//IiOnZew48umUr/bVazYHY7Rg8fzNBhH7DRxm1jsqiGejHm/M2Z9EoXOndt4M8PvM3EJ7vx/jvVcZtxVY9iN4qI76Yvr5T0ANA9Il5ej3M0zv7TPcu+k4FFEbGdpJHAr0nus2ylgn0PX8j/HPupcgdSUhJ07po8aFtfJxrqhATbfWZlzvorl9cSAauW19KtRwO17YIB2364tk7vzerZuE89SxbUtpkEuXBuexbOTf6hXbm8lmmTOtGnX13VJMhK7z4XIt+N4rvm2xcRE5s7uKQBwOHABcAPs1QZAfwsfX078GdJSlclq0gBXDj2bSLgvhv6cv9Nm6zdN2S3ZSya356ZU6rkL3geDQ0w+pDBzJzSgS98fT6f2jX3ahxfPGk+5319IF/dZUdWLKvhx1dOpabJ1e83/9OF+tWi39arSxx5Zdp0wGq2HbKSNyd2KXcoRVPtLcjf59kXwP4FHP+PwFlAtxz7+wPTACKiXtISoDcwP7OSpFOAUwA6Ud6/QGf8v0+zYE4HNu5dx6/GvsW0dzvz6vPJ1xv2xQU8Ma53WeNrKbW1cMUjb7FsSS0/P3lrprzZia0/tSpr3Ree6Ma2O67kN7e9y8wpHThn5LYM2X0ZXbslrdAFc9rx2+9tyZmXvP+xxNkWdOrSwP9eNYUrf7o5K5ZV0eWZKrgGmfOvY0Tsl2drNjlKOgKYGxEvfNIgI2JMRAyNiKHtVd7W2YI5HQBYsqA9/36wJ4N3XgZATW2w1/BFPPn3Xvk+XnU22riBnT6/jPGP5/o3EB66pRd7HbYECfoPXM1mW65m2qTkv+PyD2r46fHb8PWzZ/Hp/2p7a8LVtgv+96opPHZnT/51f49yh1M8hY5gV3grs5T/Xu8FfFHSFJLnuPeXNLZJnRmkI+KS2pHcPrSghDF9Ih07N9C5a8Pa17vuu4QpbyUt2l32Xsq0dzszf3aHcobYIhYvqGXZkqSl8+FKMfHJbmyx3Yc56/ftX8eLTyUJdNG8dkx/tyP9tvyQutXi/JMHcsDRi9jniCUtEntlCX74+2lMe6cTd46pktHrTFWQIAt6kmZDRMQ5pM9rSxoGnBkRX2tSbRxwIsmo+FHAY5V8/bFnnzp+OmYSkPzL//g9vXnhn8ktocO+sIAnxrWN1uPCOe353WlbsmaNWLMG9v3CYvY4aCl3X9WH267YhIVz2zPqwE+x2/5L+cHvp3Hc6bP53elb8u39BxMBJ/9kFhv3buDRO3ryyrMbsXRhOx6+Jfntzvzj+2w7JPtgT7XZcbflHHj0It57vROXP/wWAH/9VT/GP5ZtPLP1UZEmzJV0DdDYIx2Slv0M+BYwL63244i4L913DskAcAPw/Yh4MC0fDlwC1AJXRcRFzZ67JfJRRoI8QtL5wISIGCepE3A9sAuwEBgZEe/lO1b3mt6xR/vhpQ651Xpg6vPlDqHiHbL5zuUOoeI9Ere/EBFDN/TzHbfYIgac9oOC6r73ozPynkvSviS3F17XJEEui4jfNam7A8l8tbuRzF/7CLB9uvtt4CBgOjAeODYiXs8XWyGPGopkyYVtIuJ8SVsCm0VEwf8nRsQTwBPp659mlK8Cji70OGbWOiiKN4odEU9K2rrA6iOAmyPiQ2CypEkkyRJgUmMDTNLNad28CbKQa5CXA3sCx6bvPwAuKzBYM2urCl9yoY+kCRnbKQWeYbSklyVdI6lnWrb2zpjU9LQsV3lehSTI3SPiVGAVQEQsAqp/JMLMPpnCB2nmN96lkm5jCjj6FcC2JKusziL/bYkbrJBBmrr0ccEAkNSXqlivzMxKqZQ3ikfEnLXnkf4C3Ju+XXtnTGpAWkae8pwKaUFeCtwFbCLpApKpzi4s4HNm1lZFMopdyLYhJPXLePsl4NX09ThgpKSOkgYCg4DnSQZlBkkaKKkDMDKtm1chz2LfIOkFkinPBBwZEW+s17cxs7anSC1ISTeRzEnbR9J04DxgmKSd07NMIV0SJiJek3QryeBLPXBqRDSkxxkNPEhym881EfFac+cuZBR7S2AF8PfMsoh4v/CvaGZtTvFGsY/NUnx1nvoXkMz/0LT8PtZzwcFCrkH+g48W7+oEDATeAnZcnxOZWdtS7ZNVABARn8l8n87y890c1c3MqsZ6P2oYERMl7V6KYMysirSFFqSkzHkca4BdgZkli8jMWr8o3rPY5VRICzJzHqt6kmuSd5QmHDOrGtXegkxvEO8WEWe2UDxmVgVElQ/SSGqXzvK9V0sGZGZVopoTJMnd57sCL0oaB9wGLG/cGRF3ljg2M2utijibTzkVcg2yE8ks3/vz0f2QAThBmlluVT5Is0k6gv0qHyXGRlXwb4OZlVK1tyBrgY1YNzE2qoKvbmYlVQVZIl+CnBUR57dYJGZWPVrBglyFyJcgW/+itmZWNtXexT6gxaIws+pTzQkyIha2ZCBmVl3ayqOGZmbrpw1cgzQz2yCiOgYxnCDNrDTcgjQzy67aR7HNzDacE6SZWRZtaMJcM7P15xakmVl2vgZpZpaLE2QZRBB1q8sdRcU6bKeDyh1CKzCv3AG0CdXQgqwpdwBmVoWCZMLcQrZmSLpG0lxJr2aU9ZL0sKR30j97puWSdKmkSZJelrRrxmdOTOu/I+nEQr6GE6SZFV3jol2FbAX4GzC8SdnZwKMRMQh4NH0PcCgwKN1OAa6AJKEC5wG7A7sB5zUm1XycIM2sNKLArbnDRDwJNJ08ZwRwbfr6WuDIjPLrIvEs0ENSP+AQ4OGIWBgRi4CH+XjS/ZjWdw3SzFoFRcEXIftImpDxfkxEjGnmM5tGxKz09Wxg0/R1f2BaRr3paVmu8rycIM2s+NZvNp/5ETF0g08VEVJphoTcxTazkijiNchs5qRdZ9I/56blM4AtMuoNSMtyleflBGlmJaE1hW0baBzQOBJ9InBPRvkJ6Wj2HsCStCv+IHCwpJ7p4MzBaVle7mKbWWkUqdMr6SZgGMm1yukko9EXAbdKOhmYCnwlrX4fcBgwCVgBnATJCgmSfgGMT+udX8iqCU6QZlZ8n6z7vO6hIo7Nsetj62ZFRACn5jjONcA163NuJ0gzK40qeJLGCdLMiq7xRvHWzgnSzEpCa1p/hnSCNLPi86qGZma5eUZxM7Nc3II0M8vOgzRmZtkEUPhkFRXLCdLMSsLXIM3MsvB9kGZmuUS4i21mlotbkGZmuThBmpll5xakmVk2ATS0/gzpBGlmJeEWpJlZLh7FNjPLzi1IM7NsPN2ZmVl2AuRBGjOz7ORrkGZmWVRJF7um3AG0ZkOHLeWqp97kr/96g6+MnlPucMri9J+/xo2P/5PL73hmbdnA7T/g99eN5/Lbn+G8S1+kc9f6dT7Td7NV3PHM43z5hKktHW5FqqkJLnvoLc6/9r1yh1JE8dHz2M1tFaykCVLSFEmvSHpR0oQs+yXpUkmTJL0saddSxlNMNTXBqRfO4NzjBvKtYYPZb8Rithy0qtxhtbhH7tmc//3OLuuUnXbeG/z1ku347lF78u/H+nLU19dNhN86820mPN27JcOsaEd+cz7T3ulU7jCKTlHYVslaogW5X0TsHBFDs+w7FBiUbqcAV7RAPEUxeJcVzJzSgdnvd6S+roYn7unBnocsKXdYLe7ViT35YGn7dcr6b7WcV1/oAcB/nunNXgfMXbtvz/3mMntGZ95/t2tLhlmx+vRbzW4HLOX+G3uVO5TicwvyExsBXBeJZ4EekvqVOaaC9N6sjnkzO6x9P39We/r0qytjRJVj6rsbsed+8wDY5+A59NksaVl36lzPUSdN5cYrB5YzvIoy6uczueqX/Yg1KncoxRXJKHYhWyGy9UYl9ZL0sKR30j97puVF65mWOkEG8JCkFySdkmV/f2BaxvvpaZm1Yn88bwcOP2Y6l9z0HJ27NFBfl/w1O+4773H32C1ZtdJjgwC7H7iUxfPbMemVLuUOpTSiwK1wTXujZwOPRsQg4NH0PRSxZ1rqv6l7R8QMSZsAD0t6MyKeXN+DpMn1FIBOVMZfpgWz29N389Vr3/fpV8f8We3zfKLtmD6lK+eOSv7R7r/Vcj6373wABn9mKXsfOJdvnP4OXbvVEwGrV9dw781blDPcstnhc8vZ4+ClfO6A1+nQMejSrYGz/jSV33xvq3KHVhQtcJvPCGBY+vpa4Angf8jomQLPSuohqV9EzFrfE5Q0QUbEjPTPuZLuAnYDMhPkDCDz/44BaVnT44wBxgB0V6+KuGjx1otd6D9wNZtu8SELZrdn2IjFXHRqdfzF/qQ27rWaJQs7IAUjvzWZ+25LOgVnnfTRZejjRr3LyhXt2mxyBPjrr/rx118lV5Q+u+cyjho1t2qSI1Ds64uNvdEA/i/NCZtmJL3ZwKbp61w908pJkJK6AjUR8UH6+mDg/CbVxgGjJd0M7A4s2ZAsXw5rGsRlP+nPhTe+R00tPHRzL6a+XX0jkc0566JX+OzQRXTvUcd1Dz3F2Cu2oXPnBo4YOR2Afz3al4fv3rzMUVqLC6DwRbv6NLnLZUyaADN9rDe6zukiIk2eRVXKFuSmwF2SGs9zY0Q8IGkUQERcCdwHHAZMAlYAJ5UwnqIb/1h3xj/WvdxhlNVvzv5M1vJ7btwy7+duuHLbUoTTar38zEa8/MxG5Q6jaESsTxd7fo67XNbK0Rud09h1Tgd3G2+XKKhnWoiSJciIeA/YKUv5lRmvAzi1VDGYWRmtKc66r3l6o+OAE4GL0j/vST9StJ6phxPNrPjWr4vdnFy90fHArZJOBqYCX0nrF61n6gRpZiVRrFHsPL3RBcABWcqL1jN1gjSz0qjwp2QK4QRpZiVQ+Y8RFsIJ0syKz6sampnl5glzzcxycYI0M8sigDVOkGZmWXiQxswsNydIM7MsAmgo3qM05eIEaWYlEBBOkGZm2bmLbWaWhUexzczycAvSzCwHJ0gzsywioKGh3FF8Yk6QZlYabkGameXgBGlmlk14FNvMLKuA8I3iZmY5+FFDM7MsIoq27Gs5OUGaWWl4kMbMLLtwC9LMLBtPmGtmlp0nqzAzyy6AqIJHDWvKHYCZVaFIJ8wtZCuApOGS3pI0SdLZJY5+LbcgzawkokhdbEm1wGXAQcB0YLykcRHxelFOkIdbkGZWGsVrQe4GTIqI9yJiNXAzMKKksacUrWykSdI8YGq548jQB5hf7iAqnH+j/Crx99kqIvpu6IclPUDyvQrRCViV8X5MRIzJONZRwPCI+Gb6/nhg94gYvaHxFarVdbE/yX+0UpA0ISKGljuOSubfKL9q/H0iYni5YygGd7HNrNLNALbIeD8gLSs5J0gzq3TjgUGSBkrqAIwExrXEiVtdF7sCjWm+Spvn3yg//z55RES9pNHAg0AtcE1EvNYS5251gzRmZi3FXWwzsxycIM3McnCCLICkTpKel/SSpNck/TxLnY6SbkkfhXpO0tZlCLWsJNVK+o+ke7Ps8+8jTZH0iqQXJU3Isl+SLk1/o5cl7VqOOO0jTpCF+RDYPyJ2AnYGhkvao0mdk4FFEbEdcDHw65YNsSKcBryRY59/n8R+EbFzjvseDwUGpdspwBUtGpl9jBNkASKxLH3bPt2ajm6NAK5NX98OHCBJLRRi2UkaABwOXJWjSpv+fQo0Argu/fv2LNBDUr9yB9WWOUEWKO0+vgjMBR6OiOeaVOkPTIPktgRgCdC7RYMsrz8CZwG5Hq5t678PJP+oPiTpBUmnZNm/9jdKTU/LrEycIAsUEQ0RsTPJXfy7SRpS5pAqhqQjgLkR8UK5Y6lwe0fEriRd6VMl7VvugCw/J8j1FBGLgceBps+arn0cSlI7YGNgQYsGVz57AV+UNIVkppX9JY1tUqct/z4ARMSM9M+5wF0ks9RkKtsjdZadE2QBJPWV1CN93ZlkXro3m1QbB5yYvj4KeCzayF34EXFORAyIiK1JHgN7LCK+1qRam/19ACR1ldSt8TVwMPBqk2rjgBPS0ew9gCURMauFQ7UMftSwMP2Aa9OJO2uAWyPiXknnAxMiYhxwNXC9pEnAQpJE0ab591nHpsBd6bhUO+DGiHhA0iiAiLgSuA84DJgErABOKlOslvKjhmZmObiLbWaWgxOkmVkOTpBmZjk4QZqZ5eAEaWaWgxNkFZLUkM4Y86qk2yR1+QTH+lu6qhySrpK0Q566wyR9fgPOMUXSx1bAy1XepM6yfPuz1P+ZpDPXN0Zrm5wgq9PKdMaYIcBqYFTmzvRJlvUWEd9sZrH2YcB6J0izSuUEWf2eArZLW3dPSRoHvJ5OvvFbSePTuQe/DWvnJPyzpLckPQJs0nggSU9IGpq+Hi5pYjpH5qPp/I6jgB+krdd90ieQ7kjPMV7SXulne0t6KJ1b8yqg2Vl9JN2dTvLwWtOJHiRdnJY/KqlvWratpAfSzzwl6VNF+TWtTfGTNFUsbSkeCjyQFu0KDImIyWmSWRIRn5PUEfiXpIeAXYDBwA4kT3+8DlzT5Lh9gb8A+6bH6hURCyVdCSyLiN+l9W4ELo6IpyVtSbLo0qeB84CnI+J8SYeTzBXZnG+k5+gMjJd0R0QsALqSPK3zA0k/TY89mmQhrFER8Y6k3YHLgf034Ge0NswJsjp1Tqdmg6QFeTVJ1/f5iJiclh8MfLbx+iLJ5BGDgH2BmyKiAZgp6bEsx98DeLLxWBGxMEccBwI7ZEz72F3SRuk5vpx+9h+SFhXwnb4v6Uvp6y3SWBeQTK92S1o+FrgzPcfngdsyzt2xgHOYrcMJsjqtTKdmWytNFMszi4DvRcSDTeodVsQ4aoA9ImJVllgKJmkYSbLdMyJWSHoC6JSjeqTnXdz0NzBbX74G2XY9CHxHUnsASduns8w8CRyTXqPsB+yX5bPPAvtKGph+tlda/gHQLaPeQ8D3Gt9I2jl9+STw1bTsUKBnM7FuTLJcw4r0WmLmchc1JLMDkR7z6YhYCkyWdHR6DknaqZlzmH2ME2TbdRXJ9cWJkl4F/o+kR3EX8E667zrgmaYfjIh5JGum3CnpJT7q4v4d+FLjIA3wfWBoOgj0Oh+Npv+cJMG+RtLVfr+ZWB8A2kl6A7iIJEE3Wk4ygfGrJNcYz0/LjwNOTuN7jWQ5A7P14tl8zMxycAvSzCwHJ0gzsxycIM3McnCCNDPLwQnSzCwHJ0gzsxycIM3Mcvj/vWg0OFOYTBEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.704683434518647\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         3.0       0.56      0.07      0.12      5864\n",
      "         4.0       0.71      0.98      0.82     16208\n",
      "         5.0       0.74      0.02      0.03       991\n",
      "\n",
      "    accuracy                           0.71     23063\n",
      "   macro avg       0.67      0.36      0.33     23063\n",
      "weighted avg       0.67      0.71      0.61     23063\n",
      "\n",
      "0.7070197464122002\n"
     ]
    }
   ],
   "source": [
    "# knn using kfold to avoid overfitting\n",
    "k = 5\n",
    "kf_CV = StratifiedKFold(n_splits = k, shuffle = True, random_state = 10)\n",
    "\n",
    "# Scale X data, we scale the data because it helps the algorithm to converge\n",
    "# and helps the algorithm to not be greedy with large values, here mainly the\n",
    "# page numbers are scaled\n",
    "accuracy = []\n",
    "all_y_predictions = []\n",
    "all_y_test = []\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train)\n",
    "scaled_df = scaler.transform(df_train)\n",
    "\n",
    "# Enter a loop to run K folds of cross-validation\n",
    "for train_idx, test_idx in kf_CV.split(scaled_df, y_train):\n",
    "    # train-test split\n",
    "    X_train, X_test = scaled_df[train_idx], scaled_df[test_idx]\n",
    "    Y_train, Y_test = y_train[train_idx], y_train[test_idx]\n",
    "\n",
    "    # hyperparameters here refer to the GridSearchCV in sklearn, later\n",
    "    # discussed in report\n",
    "    model = KNeighborsClassifier(n_neighbors=27, weights='distance')\n",
    "    trained_model = model.fit(X_train, Y_train)\n",
    "    y_pred_knn = trained_model.predict(X_test)\n",
    "    all_y_predictions.extend(y_pred_knn)\n",
    "    all_y_test.extend(Y_test)\n",
    "    accuracy.append(trained_model.score(X_test, Y_test))\n",
    "    print(accuracy[-1])\n",
    "\n",
    "    # plot the confusion metrix of each prediciton\n",
    "    cm = confusion_matrix(Y_test, # test data\n",
    "                      y_pred_knn, # predictions\n",
    "                      labels=sorted(list(y_train.unique())) # class labels from the knn model\n",
    "                     )\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, # pass through the created confusion matrix\n",
    "                              display_labels=sorted(list(y_train.unique())) # class labels from the knn model \n",
    "                             )\n",
    "\n",
    "    disp.plot()\n",
    "\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "print(classification_report(all_y_test, all_y_predictions))\n",
    "print(np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df_train)\n",
    "scaled_test = scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(scaled_test)\n",
    "submission_df = pd.Series(preds)\n",
    "submission_df.index = submission_df.index + 1\n",
    "submission_df.to_csv('knn_results.csv', index_label='id', header=['rating_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    5708\n",
       "3.0      52\n",
       "5.0       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_df, y_train, test_size=0.9, stratify=y_train, random_state=30027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     C     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer \u001b[39m=\u001b[39m BayesianOptimization(f\u001b[39m=\u001b[39mobjective_function, pbounds\u001b[39m=\u001b[39mpbounds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Set the number of iterations or maximum number of function evaluations\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mmaximize(init_points\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Get the best result\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m best_result \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mmax\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bayes_opt/bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuggest(util)\n\u001b[1;32m    309\u001b[0m     iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 310\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobe(x_probe, lazy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer \u001b[39mand\u001b[39;00m iteration \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    313\u001b[0m     \u001b[39m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_bounds(\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bayes_opt/bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue\u001b[39m.\u001b[39madd(params)\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_space\u001b[39m.\u001b[39;49mprobe(params)\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch(Events\u001b[39m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bayes_opt/target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_as_array(params)\n\u001b[1;32m    235\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys, x))\n\u001b[0;32m--> 236\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constraint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(x, target)\n",
      "\u001b[1;32m/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb Cell 38\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(C)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m svm \u001b[39m=\u001b[39m LinearSVC(C\u001b[39m=\u001b[39mC, max_iter\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)  \u001b[39m# Increase max_iter\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Perform cross-validation and calculate the mean accuracy\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m accuracy \u001b[39m=\u001b[39m cross_val_score(svm, X_train, Y_train, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Return the negative accuracy since Bayesian optimization maximizes the objective function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wangzeyu/Desktop/ML/project2/data_modelling_walkthrough.ipynb#X63sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39maccuracy\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m    269\u001b[0m         X,\n\u001b[1;32m    270\u001b[0m         y,\n\u001b[1;32m    271\u001b[0m         scorers,\n\u001b[1;32m    272\u001b[0m         train,\n\u001b[1;32m    273\u001b[0m         test,\n\u001b[1;32m    274\u001b[0m         verbose,\n\u001b[1;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    276\u001b[0m         fit_params,\n\u001b[1;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/svm/_classes.py:257\u001b[0m, in \u001b[0;36mLinearSVC.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    254\u001b[0m check_classification_targets(y)\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_, n_iter_ \u001b[39m=\u001b[39m _fit_liblinear(\n\u001b[1;32m    258\u001b[0m     X,\n\u001b[1;32m    259\u001b[0m     y,\n\u001b[1;32m    260\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintercept_scaling,\n\u001b[1;32m    263\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty,\n\u001b[1;32m    265\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual,\n\u001b[1;32m    266\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    267\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    268\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    269\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m    270\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_class,\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[1;32m    272\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[39m# Backward compatibility: _fit_liblinear is used both by LinearSVC/R\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m# and LogisticRegression but LogisticRegression sets a structured\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39m# `n_iter_` attribute with information about the underlying OvR fits\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m# while LinearSVC/R only reports the maximum value.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m n_iter_\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/svm/_base.py:1205\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1202\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m   1204\u001b[0m solver_type \u001b[39m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1205\u001b[0m raw_coef_, n_iter_ \u001b[39m=\u001b[39m liblinear\u001b[39m.\u001b[39;49mtrain_wrap(\n\u001b[1;32m   1206\u001b[0m     X,\n\u001b[1;32m   1207\u001b[0m     y_ind,\n\u001b[1;32m   1208\u001b[0m     sp\u001b[39m.\u001b[39;49misspmatrix(X),\n\u001b[1;32m   1209\u001b[0m     solver_type,\n\u001b[1;32m   1210\u001b[0m     tol,\n\u001b[1;32m   1211\u001b[0m     bias,\n\u001b[1;32m   1212\u001b[0m     C,\n\u001b[1;32m   1213\u001b[0m     class_weight_,\n\u001b[1;32m   1214\u001b[0m     max_iter,\n\u001b[1;32m   1215\u001b[0m     rnd\u001b[39m.\u001b[39;49mrandint(np\u001b[39m.\u001b[39;49miinfo(\u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmax),\n\u001b[1;32m   1216\u001b[0m     epsilon,\n\u001b[1;32m   1217\u001b[0m     sample_weight,\n\u001b[1;32m   1218\u001b[0m )\n\u001b[1;32m   1219\u001b[0m \u001b[39m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[39m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[39m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[39m# srand supports\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m n_iter_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the objective function, not used in our project\n",
    "def objective_function(C):\n",
    "    # Instantiate the LinearSVC classifier with the hyperparameter\n",
    "    svm = LinearSVC(C=C, max_iter=10000)  # Increase max_iter\n",
    "    \n",
    "    # Perform cross-validation and calculate the mean accuracy\n",
    "    accuracy = cross_val_score(svm, X_train, Y_train, cv=5).mean()\n",
    "    \n",
    "    # Return the negative accuracy since Bayesian optimization maximizes the objective function\n",
    "    return -accuracy\n",
    "\n",
    "# Define the bounds of the search space\n",
    "pbounds = {'C': (0.001, 5)}\n",
    "\n",
    "# Create an instance of the BayesianOptimization class\n",
    "optimizer = BayesianOptimization(f=objective_function, pbounds=pbounds)\n",
    "\n",
    "# Set the number of iterations or maximum number of function evaluations\n",
    "optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "# Get the best result\n",
    "best_result = optimizer.max\n",
    "\n",
    "print(\"Best parameter C:\", best_result['params']['C'])\n",
    "print(\"Best negative accuracy:\", -best_result['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X) #X_meta is the output (y_hat) of the base classifiers\n",
    "        self.metaclassifier.fit(X_meta, y) #output of the base classifiers is the input for the meta classifier\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0] # check that the number of rows yhats matches the number of rows in the input data X\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking \n",
    "classifiers = [DummyClassifier(strategy='most_frequent'),\n",
    "          KNeighborsClassifier(n_neighbors=27, weights='distance'),\n",
    "          LogisticRegression(max_iter=1000, C=0.001)]\n",
    "\n",
    "titles = ['Zero_R',\n",
    "          '27-nearest neighbour',\n",
    "          'Logistic Regression']\n",
    "\n",
    "param_grid = {# 'LinearSVC__C': [1, 0.1, 0.01, 10],\n",
    "               'final_estimator__n_neighbors': (5, 10)}\n",
    "\n",
    "meta_classifier_knn = KNeighborsClassifier(n_neighbors = 33, weights='distance')\n",
    "stacker_knn = StackingClassifier(classifiers, meta_classifier_knn)\n",
    "\n",
    "# meta_classifier_lgr = LogisticRegression()\n",
    "# stacker_lgr = StackingClassifier(estimators=list(zip(titles, classifiers)), final_estimator=meta_classifier_lgr, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df_train)\n",
    "scaled_test = scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero_R Accuracy: 0.7027964448298287\n",
      "27-nearest neighbour Accuracy: 0.709299804899198\n",
      "Logistic Regression Accuracy: 0.7060481248645133\n",
      "\n",
      "Stacker Accuracy (KNN): 0.709299804899198\n",
      "Zero_R Accuracy: 0.7027964448298287\n",
      "27-nearest neighbour Accuracy: 0.7056145675265554\n",
      "Logistic Regression Accuracy: 0.6993279861261652\n",
      "\n",
      "Stacker Accuracy (KNN): 0.7056145675265554\n",
      "Zero_R Accuracy: 0.7027964448298287\n",
      "27-nearest neighbour Accuracy: 0.7056145675265554\n",
      "Logistic Regression Accuracy: 0.6982440927812703\n",
      "\n",
      "Stacker Accuracy (KNN): 0.7053977888575764\n",
      "Zero_R Accuracy: 0.7027320034692107\n",
      "27-nearest neighbour Accuracy: 0.7064180398959237\n",
      "Logistic Regression Accuracy: 0.7049002601908065\n",
      "\n",
      "Stacker Accuracy (KNN): 0.7066348655680833\n",
      "Zero_R Accuracy: 0.7027320034692107\n",
      "27-nearest neighbour Accuracy: 0.7079358196010408\n",
      "Logistic Regression Accuracy: 0.7109713790112749\n",
      "\n",
      "Stacker Accuracy (KNN): 0.7079358196010408\n"
     ]
    }
   ],
   "source": [
    "# stacking using k fold\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(scaled_df, y_train):\n",
    "# Split the data into training and testing sets\n",
    "    X_train, X_test = scaled_df[train_index], scaled_df[test_index]\n",
    "    Y_train, Y_test = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    for title, clf in zip(titles,classifiers):\n",
    "        clf.fit(X_train,Y_train)\n",
    "        print(title, \"Accuracy:\",clf.score(X_test,Y_test))\n",
    "\n",
    "    stacker_knn.fit(X_train, Y_train)\n",
    "    print('\\nStacker Accuracy (KNN):', stacker_knn.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = stacker_knn.predict(scaled_test)\n",
    "submission_df = pd.Series(preds)\n",
    "submission_df.value_counts()\n",
    "submission_df.index = submission_df.index + 1\n",
    "submission_df.to_csv('submit.csv', index_label='id', header=['rating_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    5703\n",
       "3.0      55\n",
       "5.0       8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero_R Accuracy: 0.7027964448298287\n",
      "27-nearest neighbour Accuracy: 0.7064816822024713\n",
      "Logistic Regression Accuracy: 0.7069152395404292\n",
      "\n",
      "Stacker Accuracy (KNN): 0.7062649035334922\n"
     ]
    }
   ],
   "source": [
    "# train test split used to draw the correlation matrix\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_df, y_train, test_size=0.2, stratify=y_train, random_state=30027)\n",
    "predictions = {}\n",
    "for title,clf in zip(titles,classifiers):\n",
    "    clf.fit(X_train,Y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "    predictions[title] = prediction\n",
    "    print(title, \"Accuracy:\",clf.score(X_test,Y_test))\n",
    "\n",
    "base_df = pd.DataFrame(predictions)\n",
    "    \n",
    "stacker_knn.fit(X_train, Y_train)\n",
    "print('\\nStacker Accuracy (KNN):', stacker_knn.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "correlation_matrix = base_df.corr()\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, square=True)\n",
    "plt.title(\"Correlation Matrix of Base Models\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
